\documentclass[twocolumn]{article}

\usepackage[backend=biber]{biblatex}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{expl3}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{todonotes}
\usepackage{tabularx}
\usepackage[inline]{enumitem}
\usepackage{float}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage{algpseudocode}

% Setup matplotlib pgf plots
\usepackage{pgf}
\def\mathdefault#1{#1}
\everymath=\expandafter{\the\everymath\displaystyle}
\makeatletter\@ifpackageloaded{underscore}{}{\usepackage[strings]{underscore}}\makeatother

% Layout
\setuptodonotes{inline}
\renewcommand*{\bibfont}{\footnotesize}
\newfloat{algorithm}{t}{lop}

\addbibresource{references.bib}
\input{include/math.tex}
\input{include/tensorviz.tex}

% Metadata
\title{Block-Term Tensor Discriminant Analysis for Brain-Computer Interfacing}
\author{Arne Van Den Kerchove}

\begin{document}

\maketitle

\section{Introduction}
Brain-computer interfaces (BCIs) can replace, supplement, or enhance neural
communication pathways by enabling direct interaction between the brain and
external devices, with applications in the development of neuroprosthetics and
assistive technologies, among other fields~\cite{NicolasAlonso2012}.
To achieve their functionality, BCIs record and process neural data obtained
through neuroimaging techniques, with the electroencephalogram (EEG) being the
most commonly used method.

EEG data, like most neural signal acquisition modalities used for BCIs,
naturally exist as multi-channel time series, capturing information in both
spatial and temporal domains.
Common preprocessing transformations, such as time-frequency transformation,
time-binning, or integrating information across multiple subjects or conditions,
can further expand the data into additional analysis domains.
This can result in high-dimensional datasets, yet with a certain decoupling
between the domains.
Therefore, the intrinsic multiway structure of neural data~\cite{Erol2022} is
well-suited for representation as multiway arrays, or \emph{tensors}, which
provide a structured data representation that counteracts some of the drawbacks
resulting from this high dimensionality.

Machine learning techniques dealing with this datatype are referred to a tensor
methods.
Tensor methods can consider each analysis domain (tensor \emph{mode}) separately to
reduce a given problem into partial, per mode problems.
This has given rise to efficient dimensionality reduction techniques, such as
the Higher-Order Singular Value Decomposition
(\textsc{hosvd})~\cite{DeLathauwer2000,SoleCasals2018} or Canonical Polyadic Decomposition
(\textsc{cpd})\cite{Nazarpour2006}\todo{cite original}.

The former are all examples of unsupervised techniques with applications in EEG
processing.Given the specific task-related output required in the BCI applications, however,
supervised feature extraction and machine learning techniques are often of
interest in this field.

One approach is to incorporate some assumptions of the tensor structure of the
data directly into the estimation of parameters of classic linear machine
learning methods, such as in Linear Discriminant Analysis or beamforming.
Advances have been made by leveraging the decoupling of the spatial and temporal
domains in EEG event-related potential (ERP) classification using Spatiotemporal
Discriminant analysis~\cite{Li2010,Zhang2013} or methods regularizing covariance
matrix estimation~\cite{Kerchove2022,Sosulski2022}.\todo{include this
paragraph?}

A more structured approach to the same problem is to design a supervised
tensor dimensionality tensor method that optimizes discriminability between the
extracted features, as is the case for Higher Order Discriminant
Analysis (\textsc{hoda})~\cite{Yan2005,Phan2010,Froelich2018}.
These extracted features can subsequently be further classified, most commonly
using LDA or a support vector machine (SVM) to obtain predictions.
Variants of \textsc{hoda} have been applied to BCI problems such as
ERP~\cite{Onishi2012,Higashi2016} and motor imagery (MI)~\cite{Liu2015,Cai2021}.
Recent adaptations improve on these results by using suited objective
functions and regularization, such as in Higher order spectral regression
discriminant analysis~\cite{Jamshidi2017}, Spatiotemporal Linear
Feature~\cite{Aghili2023}, Oscillatory source Tensor Discriminant
Analysis~\cite{Jorajuria2022}.

The previous methods adhere to the \textsc{tucker} tensor decomposition
structure, meaning that they reduce input tensors of size
$(D_1,D_2,\ldots,D_K)$ to a smaller tensor of size $(r_1,r_2,\ldots,r_K)$ with
each $r_k\leq D_k$, similar to \textsc{hosvd}.
While effective, other structures such as the \textsc{parafac} employed in
\textsc{cpd}, where a tensor is decomposed into a sum of rank-1 tensors,
might be more appropriate to represent the neural data of interest.
Discriminant tensor features can also be extracted
in the \textsc{parafac} structure with, for instance using manifold
optimization~\cite{Froelich2018}.

Nevertheless, the \textsc{parafac} structure might still not be able to
efficiently represent all relevant information in a compressed format.
The block-term tensor structure is a generalization of the \textsc{tucker} and
\textsc{parafac} structures, and can be calculated in an unsupervised way using
the Block-term Tensor Decomposition
(\textsc{btd})~\cite{DeLathauwer2008,DeLathauwer2008a,DeLathauwer2008b,Rontogiannis2021}.
\textsc{btd}, of which the \textsc{hosvd} and \textsc{cpd} are special cases,
represents a tensor a sum of \textsc{tucker} terms.
Research has shown that this more flexible structure can improve BCI performance
when adapted to supervised methods, such as in Higher-Order Parial Least
Squares~\cite{Camarrone2018} or Block-Term Tensor Regression (\textsc{bttr})~\cite{Faes2022,Faes2022b}
\textsc{bttr} has been adapted into a classification variant, named Block-Term
Tensor Classification (\textsc{bttc})~\cite{Camarrone2021}, but since features
are not directly optimized for class separability but rather regressed towards
a dummy independent variable, results can be improved upon and the method
cannot be extended to a multi-class setting.
Furthermore, structures employed in \textsc{hopls}, \textsc{bttdr} and
\textsc{bttc} are still more constrained than what could be achieved with a
full block-term tensor structured decomposition optimized for discriminability.

\todo{talk about spectrum-weighted tda~\cite{Huang2020}, criticize structure}

In this work, we propose the following contributions:
\begin{enumerate*}[label={\arabic*)}]
  \item first, we develop a forward model for \textsc{hoda} to reconstruct a
    given input tensor from the extracted features.
  \item This allows us to introduce a state-of-the-art BCI classification method based on the
    block-term tensor structure, named Block-Term Tensor Discriminant Analysis
    (\textsc{BTTDA}) and
  \item evaluate this decoder together with other tensor methods and the
    special \textsc{parafac}-structured case on an extensive benchmark of BCI
    datasets for ERP and MI decoding).
\end{enumerate*}

\section{Methods}

\subsection{Notation}
Tensors are indicated as bold underlined letters $\ten{X}$, matrices as bold
letters $\mat{U}$, fixed scalars as uppercase letters $K$ and variable
scalars as lower case letters $k$.
The $n^\text{th}$ sample of a tensor dataset with $N$ samples is written as
$\ten{X}(n)$.
A tensor $\ten{X}\in \mathbb{R}^{D_1\times D_2 \times \cdots \times D_K}$ can be unfolded in mode
$k$ to a matrix $\mat{X}_k\in\mathbb{R}^{(D_k\times\prod_{j\neq k}^K D_j)}$.
matrix
The tensor-matrix product of tensor $\ten{X}$ with matrix $\mat{U}$ along a
given mode $k$ is written as $\ten{X}\mpr{\mat{U}}{k}$. For ease of notation, let
$\ten{X}\mmpr{\mat{U}} =
	\ten{X}\mpr{\mat{U}}{1}\mpr{\mat{U}}{2}\cdots\mpr{\mat{U}}{K}$.
When skipping one of the modes $k$, this is
written as $\ten{X}\mmprs{\mat{U}}{k} =
	\ten{X}\mpr{\mat{U}}{1}\mpr{\mat{U}}{2}\cdots\mpr{\mat{U}}{k-1}\mpr{\mat{U}}{k+1}\ldots\mpr{\mat{U}}{K}$.
%The Kronecker product is noted as $\otimes$, covariance matrices are indicated  with $\mat{\Sigma}$.

\subsection{Higher-Order Discriminant Analysis (\textsc{hoda})}
Higher Order Discriminant Analysis (\textsc{hoda})~\cite{Phan2010} is a tensor
feature extraction technique. For a $K^{th}$ order input tensor $\ten{X}$ of
shape $(D_1,D_2,\ldots,D_K)$, \textsc{hoda} finds projection matrices $\mat{U_k}$ for each mode $k$
that project $\ten{X}$ a core tensor $\ten{G}$, usually with lower
dimensionality $(r_1,r_2,\ldots,r_K)$ using tensor-matrix mode products
\begin{equation}
	\ten{G}  = \ten{X}\mmpr{\mat{U}}
	\label{eq:hoda-backward}
\end{equation}
visualized in Figure~\ref{fig:hoda-backward}.
\begin{figure}
	\centering
	\input{figures/hoda_backward.tikz.tex}
	\caption{A visualization of the multilinear projection learnt by Higher Order
		Discriminant Analysis (\textsc{hoda}) for a dataset of $N$ second order tensors
		$\ten{X}$ of shape $(D_1,D_2)$.
		\textsc{hoda} finds projection matrices $\mat{U}_k$ such that maximal
		discriminability between classes is achieved in the projected core tensors
		$\ten{G}$ with reduced dimensionality $(r_1,r_2)$.}
	\label{fig:hoda-backward}
\end{figure}
Analogous to the \textsc{hosvd}, \textsc{hoda} is a dimensionality
reduction decomposition that results in a dense core tensor $\ten{G}$, and
imposes an orthogonality constraint on $\mat{U}_k$ to ensure uniqueness.
However, while for the \textsc{hosvd} decomposition the projection matrices
are chosen to minimize the reconstruction error, the projection matrices
$\mat{U}_k$ of \textsc{hoda} are optimized for maximal discriminability between
$\ten{G}(n)$ belonging corresponding classes with labels $c_n$.
This is a desirable property in a classification setting where samples are
high-dimensional tensors.

\textsc{hoda} optimizes discriminability in the Fisher sense, by optimizing the
Fisher ratio $\phi$ between the core tensors $\ten{G}(n)$
\begin{equation}
  \phi = \frac{\sum_c^CN_c\left\lVert\bar{\ten{G}}(c)-\bar{\bar{\ten{G}}}\right\rVert_F^2}
	{\sum_n^N\left\lVert\ten{G}(n)-\bar{\ten{G}}(c_n)\right\rVert_F^2}
	\label{eq:fisher}
\end{equation}
for $C$ classes with each $N_c$ samples. $\bar{\ten{G}}(c)$ is the mean of core
tensors of class $c$, and $\bar{\bar{\ten{G}}}$ the mean of
these class mean core tensors.
The goal is now to find the optimal projection matrices
\begin{equation}
  \left\{\mat{U}^*\right\} = \argmax_{\{\mat{U}\}}\phi
\end{equation}
which can be solved by the iterative algorithm in Algorithm~\ref{alg:hoda}.
\begin{algorithm}
  \caption{Higher-order Discriminant Analysis (\textsc{hoda}) backward solution}
  \label{alg:hoda}
  \input{include/alg_hoda.tex}
\end{algorithm}
To start, $\mat{U}_k$ are initialized to orthogonal matrices, e.g. as random
orthogonal matrices, by a per mode Singular Value Decomposition (\textsc{svd}),
or as the partial \textsc{hosvd} of the dataset.
At each iteration, the algorithm loops trough the modes and fixes all the
projection of all but $\mat{U}_k$ corresponding to mode $k$.
It then finds a partial core tensor
\begin{equation}
  \ten{G}_{-k}=\ten{X}\mmprs{\mat{U}}{k}
\end{equation}
Subsequently, a new projection matrix $\mat{U}_k$ can be found analogous to Linear
Discriminant Analysis by constructing the within-class scatter matrix
\begin{equation}
  \mat{S}_{-k,\text{w}} = \sum_n^N\tilde{\mat{G}}_{-k,k}(n)\cdot\tilde{\mat{G}}_{-k,k}^\intercal(n)
\end{equation}
with $\tilde{\ten{G}}_{-k}(n) = \ten{G}_{-k}(n) - \bar{\ten{G}}_{-k}(c_n)$,
and the between-class scatter matrix
\begin{equation}
  \mat{S}_{-k,\text{b}} =
  \sum_c^CN_c\tilde{\bar{\mat{G}}}_{-k,k}(c)\cdot\tilde{\bar{\mat{G}}}_{-k,k}^\intercal(c)
\end{equation}
with $\tilde{\bar{\ten{G}}}_{-k}(c) = \bar{\ten{G}}_{-k}(c) - \bar{\bar{\ten{G}}}_{-k}$,
and solving the generalized eigenvalue problem
\begin{equation}
  \mat{S}_{-k,\text{b}}\mat{U}_k = \mat{\lambda}\mat{S}_{-k,\text{w}}\mat{U}_k
\end{equation}
The iterative process halts after a fixed amount or iterations, or when the
update of each $\mat{U}_k$ is lower than a predetermined threshold.

To apply \textsc{hoda} in a classification setting, the projections $\mat{U}_k$
can first be learned on a training dataset
$\left\{\ten{X}^\text{train}(m)\right\}_m^M$ with known class labels, and
training features $\left\{\ten{G}^\text{train}(m)\right\}_m^M$ can be extracted.
Next, these training features are reshaped (vectorized) into vectors
$\left\{\vec{g}^\text{train}(m) \in \mathbb{R}^{\prod_k^Kr_k}\right\}_m^M$ and a classifier is
trained on these features and the corresponding class labels.
Finally, the learnt projections $\mat{U}_k$ can be applied to unseen testing
dataset $\left\{\ten{X}(n)^\text{test}\right\}_n^N$ and the exctracted features
$\left\{\ten{G}^\text{test}(n)\right\}_n^N$ vectorized into
$\left\{\vec{g}^\text{train}(n)\right\}_n^N$ can be
classified by the trained classifier.\todo{remove set notations for brevity}
To avoid overfitting and improve performance in low sample size settings, the
\textsc{HODA} problem can be regularized by shrinking the the partial
whithin-class scatter matrix~\cite{Phan2010} with a shrinkage factor
$\alpha_k$ at each step such that the generalized eigenvalue problem becomes
\begin{equation}
	\mat{S}_b^{(-k)}\mat{U}_k =
\mat{\lambda}\left[(1-\alpha_k)\mat{S}_w^{(-k)}-\alpha_k\mat{I}\right]\mat{U}_k
\end{equation}
Like in Linear Discriminant Analysis, the shrinkage parameter $\alpha_k$ can
also be estimated in a data driven way in \textsc{hoda}~\cite{Jorajuria2022},
e.g., using the Ledoit-Wolf procedure~\cite{Ledoit2003}.


\subsection{A forward model for \textsc{hoda}}
As a prerequisite for our proposed \textsc{bttda} model, we must first find a
way to reconstruct the original data tensor $\ten{X}$ as accurately as possible
from $\ten{G}$ after dimensionality reduction.
In neuroimaging, this is referred to as a \emph{forward model}.
While a \emph{backward model} extracts latent sources or properties from the observed
data, which can be optimized for tasks like regression or classification or
informed by prior knowledge about source propagation, a forward model is a
generative model that expresses the observed data in function of some latent
properties or sources that are given.
\todo{interpretability and generative model}

The \textsc{hoda} projection in Equation~\ref{eq:hoda-backward} is an example
of a backward model.
A straightforward and computationally efficient candidate for a corresponding
forward model is
\begin{equation}
	\ten{X} = \ten{G}\mmpr{\mat{A}^\intercal} + \ten{\mathbfcal{E}}
	\label{eq:hoda-forward}
\end{equation}
with \textit{activation patterns} $\mat{A}_k \in \mathbb{R}^{D_k\times r_k}$
and error term $\ten{\mathbfcal{E}}$.
\begin{figure}
	\centering
	\input{figures/hoda_forward.tikz.tex}
	\caption{The forward projection for \textsc{hoda}. Leveraging activation
		patterns $A_k$, the original tensor $\ten{X}$ can approximately be
		reconstructed from projected core tensor of latent factors $\ten{G}$. $A_k$ are chosen such
		that the variability captured in the latent factors is maximally expressed in
		the reconstructed tensor and not in the error term.}
	\label{fig:hoda-forward}
\end{figure}

A forward model should make sure that reconstruction error is minimized and
variation captured in the latent factors is maximally captured by the forward
projection term $\ten{G}\mmpr{\mat{A}^\intercal}$, and not by the error term
$\ten{\mathbfcal{E}}$~\cite{Haufe2014}.
Hence, we aim to minimize the expected value of the cross-covariance between
the noise term and the latent factors\todo{consistently use 'core tensor' or
'latent factors'}
\begin{equation}
  \left\{\mat{A}^*\right\} = \arg\min_{\{\mat{A}\}}\text{E}\left[\text{vec}\left({\ten{\mathbfcal{E}}(n)}\right)\text{vec}\left({\ten{G}(n)}\right)\right]_n
\end{equation}
or, equivalently~\cite{Parra2005,Haufe2014},
\begin{equation}
  \left\{\mat{A}^*\right\} = \arg\min_{\{\mat{A}\}}\sum_n^N\left(\ten{X}(n) - \ten{G}(n)\mmpr{\mat{A}}\right)^2
\end{equation}
This least squares tensor approximation problem can be solved efficiently using the
alternating least squares (ALS) algorithm~\cite{Comon2009},
iteratively fixing all but one of the activation patterns such that
\begin{equation}
  \mat{A}_k = \arg\min_{\mat{A}_k}
  \sum_n^N\left[\mat{X}_k(n) -
  \mat{A}_k\left(\ten{G}(n)\mmprs{\mat{A}}{k}\right)_k\right]^2
\end{equation}
\todo{check multiplication dimensionalities here and in algorithm}
at every iteration, which can be solved directly by ordinary least squares.
The activation patterns are initialized to the weights $\{\mat{U}\}$ of the
backward model.
Similar to fitting the backward model, the iterative process for the forward
model halts after a fixed amount of iterations or when the update of each
$\mat{A}_k$ is lower than a predetermined threshold.
The full algorithm to determine the \textsc{hoda} forward projection is listed
in Algorithm~\ref{alg:hoda-fw}.
\begin{algorithm}
  \caption{Higher-order Discriminant Analysis (\textsc{hoda}) forward solution}
  \label{alg:hoda-fw}
  \input{include/alg_hoda_fw.tex}
\end{algorithm}


\subsection{Block-Term Tensor Discriminant Analysis (\textsc{bttda})}
After defining the forward model, we can construct our proposed block-term
tensor model. Assuming the core tensors $\ten{G}$
obtained by the backward projection of \textsc{hoda} do not achieve perfect
class separation, the error term $\ten{\mathbfcal{E}}$ in
Equation~\ref{eq:hoda-forward} should still contain some discriminative
information, which can be exploited to improve classifier performance.
We thus extend the \textsc{hoda} feature extraction scheme with backward an
forward models defined in respectively Equations~\ref{eq:hoda-backward}
and~\ref{eq:hoda-forward} to Block-Term Tensor Discriminant Analysis
(\textsc{bttda}), with a forward model given by
\begin{align}
  \ten{X} & = \sum_b^B\ten{G}^{(b)}\mmpr{\mat{U}^{(b)}} + \ten{\mathbfcal{E}}
  \label{eq:bttda-forward}
\end{align}
which extracts $B$ core tensors $\ten{G}^{(b)}$ from input tensor $\ten{X}$
until error term $\ten{\mathbfcal{E}}$ remains.
Figure~\ref{fig:bttda} further illustrates the \textsc{bttda} model.
\begin{figure*}
	\centering
	\input{figures/bttda.tikz.tex}
  \caption{A forward model for Block-Term Tensor Discriminant Analysis
  (\textsc{bttda}). \textsc{bttda} can extract more features
 than \textsc{hoda} by iteratively finding a core tensor $\ten{G}^{(b)}$ in a
 deflation scheme.
 The \textsc{hoda} backward projection is first applied. Next, the
 input data is reconstructed via the \textsc{hoda} forward model and the
 difference between the two is found.
 Finally, this process is repeated with this difference as input data, until a
 desired number of blocks $B$ has been found.}
  \label{fig:bttda}
\end{figure*}

Since \textsc{bttda} is specified as a forward model, a backward modelling
procedure is required which finds the core tensors $\ten{G}^{(b)}$ given $\ten{X}$ for
\textsc{bttda} to be useful as a feature extraction method.
The extracted features represented by the core tensors $\ten{G}^{(b)}$ can be
computed through a deflation scheme summarized in Algorithm~\ref{alg:bttda}.
\begin{algorithm}
  \caption{Block-term Tensor Discriminant Analysis (\textsc{bttda})}
  \label{alg:bttda}
  \input{include/alg_bttda.tex}
\end{algorithm}
For each block $b$, the core tensor is extracted using the \textsc{hoda} backward
projection in from a residual error term
$\ten{\mathbfcal{E}}^{(b)}$
\begin{equation}
  \ten{G}^{(b)} = \ten{\mathbfcal{E}}^{(b)}\mmpr{\mat{U}^{(b)}}
\end{equation}
This residual error term is calculated by finding the difference between the
previous error and its reconstruction after backward and forward \textsc{hoda}
projection
\begin{equation}
  \ten{\mathbfcal{E}}^{(b+1)} = \ten{\mathbfcal{E}}^{(b)} - \ten{G}^{(b)}
  \mmpr{\mat{A}^{\intercal(b)}}
\end{equation}
with $\ten{\mathbfcal{E}}^{(1)}=\ten{X}$.

The resulting extracted feature tensors can be flattened and concatenated into
one feature vector, so that they can be classified in a similar manner to
\textsc{hoda}.


\subsection{Model and feature selection}
Similar to the unsupervised \textsc{btd}, the performance of \textsc{bttda} is
heavily dependent on the rank $\left(r_1^{(b)}, r_2^{(b)}, \ldots,
r_K^{(b)}\right)$ and on the number of blocks $B$.
If these are not kown a priori, i.e. if they cannot be set based on insights in the
data generation process, a model selection step is necessary in order to
determine what the optimal values for $r_k^{(b)}$ and $B$ are.
Hyperparameter tuning through cross-validation is straight-forward solution to
determine these parameters, yet can be computationally expensive.
To reduce the computational cost, Algorithm~\ref{alg:model-selection} proposes a model selection algorithm that
leverages cross-validation in a greedy way per block, to iteratively find the
optimal rank for the next block given the ranks of the previous block the previous blocks.
\todo{Algorithm}
\todo{How to determine $B$}

\textsc{hoda} can extract a substantial amount of redundant features, that can
be dropped after projection and before proceeding to the classification
step~\cite{Phan2010}.
Since \textsc{bttda} extracts a set of \textsc{hoda}
feature tensors, these redundant features can stack up over the number of
blocks.
Therefore, we apply feature selection after finding all \textsc{bttda} blocks
by calculating the univariate Fisher score of the concatenated vectorized
features, and retain only those features that have  statistical significant
contribution to class separation with $p<\alpha$.
Since the extracted features are already optimized for class separation,
$\alpha$ should not be chosen too low.
The significance thershold in this work is set to $\alpha=0.95$, for all
evaluations of both \textsc{hoda} and \textsc{bttda}.


\section{Experiments}
\subsection{Datasets \& decoders}
We evaluated our proposed model in two off-line BCI decoding settings: ERP
paradigms and MI paradigms
To this purpose, we made use the openly available MOABB benchmarking
datasets~\cite{Aristimunha2023} for the paradigms in question.
\todo{crossref to appendix tables}
Within-session classification performance was assessed using stratified 5-fold
cross-validation to calculate the area under the receiver operator
characteristic curve (ROC-AUC).

To use it as a decoder, \textsc{hoda} is paired with LDA to classify the
extracted feature (HODA+LDA), with hyperparameters $r_k$.
Similarly, we implemented BTTDA+LDA with the proposed \textsc{bttda} feature
extraction with hyperparameters $r_k^{(b)}$ for each block $b$ and the number of blocks
$1\leq B\leq16$.
Finally, we also introduce PARAFACDA+LDA, BTTDA+LDA where each $r_k^{(b)}=1$, with
as only hyperparameter $B$.
Hyperparameters were determined separately for each fold using nested
stratified 5-fold cross-validation, and, for BTTDA+LDA, in conjunction with the
greedy model selection algorithm proposed in \todo{crossref}
For HODA+LDA and the \textsc{hoda} blocks in BTTDA+LDA, we choose
$r=r_1=r_2=\ldots=r_K$ with possible values $1,2,4,8,\ldots,\min_kD_k$
to reduce computational cost.
\todo{mention z-scoring}
As comparison methods, we used the methods based on
Riemannian Geometry evaluated in~\cite{Chevallier2024} on
the MOABB datasets for the ERP datasets (ERPCov+MDM, ERPCov-SVD+MDM,
XDAWNCov+MDM, XDAWN+LDA,XDAWNCov+TS+SVM).
\todo{mention MI methods}
Implementation details can be found in~\cite{Chevallier2024}.
Additionally, for the ERP paradigm, we compared our proposed methods to
Toeplitz-LDA (tLDA)~\cite{Sosulski2022} since this algorithm generally yields state-of-the-art
ERP classification performance.


\subsection{Event-Related Potentials (ERPs)}
ERPs are spatiotemporal features, with each sample forming a $2^\text{nd}$
order tensor with 2 modes ($K=2$), representing EEG channels and time samples
per epoch.
EEG signals for the evaluated datasets were recorded at the sample rate given
by Table~\ref{tab:moabb-erp} and band-pass filtered between 1Hz
and 24Hz.
The signals were then cut into epochs starting from stimulus onset with a
dataset specific length given by Table~\ref{tab:moabb-erp}.
For tLDA, HODA+LDA, PARAFAC+LDA and BTTDA+LDA decoders, epochs were downsampled to 48Hz.
Table~\ref{tab:erp-score} lists the cross-validated ROC-AUC for all evaluated
decoders.
\begin{table*}
  \footnotesize
  \input{include/score_erp.tex}
  \caption{Scores for (list) were taken from \cite{Chevallier2024}}
  \label{tab:erp-score}
\end{table*}
\todo{talk about observations rom table}
\todo{statistically compare bttda to other methods (table or text, no figures),
describe meta analysis}


\subsection{Motor Imagery (MI)}
For MI, discriminatory information is represented in the EEG data as
Event-Related Synchronizations/Desynchronizations (ERS/Ds).
Contrary to the time domain analysis performed on ERPs, ERS/Ds are only
detectable in the time-frequency domain.
Hence, for the MI task, we will transform the EEG signal into the
time-frequency domain, forming $3^\text{d}$ order tensors, with modes
representing the channels, the frequencies and time samplies and time samples.

\begin{table}
  \caption{Performances as described in \cite{Chevallier2024}}
\end{table}

\section{Discussion}
Summarize positive results

Discuss contribution over first hoda block
\begin{figure*}
  \input{figures/blocks.pgf}
\end{figure*}
\begin{itemize}
  \item Decrease of MSE with blocks
  \item increase of f ratio with blocks
  \item increase of validation score with blocks
\end{itemize}

Discuss interpretability, include plots of first blocks

better memory/time complexity for same number of features than hoda

Limitations: high computational cost of setting the parameters, future efforts
should focus on automatic parameter setting e.g. using information criteria
such as in BTTR/C


%https://sci-hub.ru/https://ieeexplore.ieee.org/abstract/document/6287946
%https://www.sciencedirect.com/science/article/abs/pii/S0031320317301875
\printbibliography

\clearpage
\appendix

%\section{Proof of Theorem~\ref{the:ap}}
\onecolumn
\section{ERP Benchmark datasets}
\begin{table*}[htp]
  \input{include/moabb_erp.tex}
  \caption{MOABB ERP benchmark datasets used for evaluation, with the number of
  subjects (\# Sub.), the number of EEG channels (\# Chan.), the number of trials
per data class (\# Trials/class), the epoch length (Epoch len.), the sampling
frequency (S. freq.) and the number of sessions per subject (\# Sessions).
Adapted from~\cite{Aristimunha2023} and~\cite{Chevallier2024}.}
  \label{tab:moabb-erp}
\end{table*}
\section{MI Benchmark datasets}
\end{document}
