\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{generic}

% TODO: use algorithmic
% TODO: affiliation and other footnotes
% TODO: eqref etc to ensure Fig. 1 format
% TODO: footnotesize in all figures and tables
% TODO: Remove textcite
% TODO: abbreviate journals/conferences in bib
% TODO: only capitalize first word in references


\usepackage{bttda-paper}

\addbibresource{references.bib}
\addbibresource{moabb_datasets.bib}

\begin{document}

\title{Block-Term Tensor Discriminant Analysis for Brain-Computer Interfacing}

\author{%
	Arne Van Den Kerchove,
	Hakim Si-Mohammed,
	Fran\c{c}ois Cabestaing,
	Marc M. Van Hulle, \IEEEmembership{Fellow, IEEE}
	\thanks{%
		Submitted for review on \today.
		This work was supported in part by the special research fund of the KU Leuven
		(GPUDL/20/031) and the University of Lille under the Global PhD Scholarship Program,
		the European Union’s Horizon Europe Marie Sklodowska-Curie Action program
		(grant agreement No. 101118964), the European Union’s Horizon 2020 research and
		innovation program (grant agreement No. 857375), the special research fund of
		the KU Leuven (C24/18/098), the Belgian Fund for Scientific Research – Flanders
		(G0A4118N, G0A4321N, G0C1522N), and the Hercules Foundation (AKUL 043).
		The authors acknowledge the support of the RITMEA project co-financed by the
		European Union with the European Regional Development Fund, the French state,
		and the Hauts-de-France Region Council.
	}
	\thanks{%
		Arne Van Den Kerchove is with the Department of Neuroscience, KU Leuven,
		BE-3000 Leuven, Belgium
		He is affiliated with the CRIStAL joint research group at University of Lille,
		CNRS, and Centrale Lille, France(e-mail: arne.vandenkerchove.com).
	}
	\thanks{%
		Hakim Si-Mohammed (e-mail: hakim.si-mohammed@univ-lille.fr) and Fran\c{c}ois
		Cabestaing (e-mail: francois.cabestaing@univ-lille.fr) are with the CRIStAL
		joint research group  (UMR 9189) at University of Lille, CNRS, and Centrale
		Lille, FR-59655 Villeneuve d'Ascq, France.
	}
	\thanks{%
		Marc M. Van Hulle is with the Department of Neuroscience, KU Leuven.
		He is a senior member of the Leuven Brain Institute and the Leuven.AI institute
		for Artificial Intelligence (email: marc.vanhulle@kuleuven.be).
	}
}

\maketitle

\begin{abstract}
	\input{abstract.txt}
\end{abstract}

\begin{IEEEkeywords}
	tensor discriminant analysis,
	brain-computer interface,
	block-term decomposition,
	multilinear decoding,
	event-related potentials,
	motor imagery

\end{IEEEkeywords}




\section{Introduction}

\Acp{bci} have the potential to bypass
defective neural pathways by providing an alternative communication channel
between the brain and an external device.
These interfaces find applications in the development of
neuroprosthetics, assistive technologies and rehabilitation~\cite{Wolpaw2020}.
To achieve their functionality, \acp{bci} record and process neural data,  with
\ac{eeg} the most popular recording method in the field.

A \ac{bci} usually operates by identifying specific, task-related activity in
the recorded \ac{eeg} data, which can then be coupled to output or actions.
This methodology often gives rise to classification problems~\cite{Lotte2018}.
Some well-known examples include the P300 speller~\cite{Krusienski2006}, where
momentary visual stimuli evoke characteristic \acp{erp} modulated by attention,
and \ac{mi}~\cite{Aggarwal2019}, where different (imagined) limb movements evoke
\acp{ersd} with different spatial patterns.
As a consequence \ac{bci} decoding (\ac{erp} vs.\ non-attended \ac{erp}, left
vs.\ right limb \ac{mi}, \ldots) involves a calibration phase training a classifier
on labeled \ac{eeg} data and an operation phase where the trained classifier is applied
to unseen \ac{eeg} data.

The duration of the calibration session should ideally be minimized to enhance
user experience.
This results in small, subject- and session-specific training datasets
which make \ac{bci} classification methods vulnerable to overfitting in the
presence of high-dimensional data.
One possible countermeasure is applying a dimensionality reduction technique
which extracts a lower-dimensional set features relevant to the classification
problem at hand.

\subsection{Tensors \& tensor methods}

Because of the multichannel time series format of \ac{eeg} and other \ac{bci} functional
neuroimaging methods, recorded data naturally exist as multiway data, capturing
information in both the spatial and the temporal domain.
Preprocessing transformations can further expand the data into additional
analytic domains.
Common examples include time-frequency transformation, time-binning, or
integrating information across multiple subjects or conditions.
This in turn results in high-dimensional datasets which are usually flattened
into a set of sample vectors, stripping the original data of its structure.
A more suited approach relies on this intrinsic multiway structure of neural
data~\cite{Erol2022} to represent the data as \emph{tensors}, multiway arrays,
with each domain corresponding to a tensor \emph{mode}.
Tensors provide a structured data representation for this highly dimensional
multiway data.
This in turn paves the way to the development of tensor methods which can
counteract some of the drawbacks of the dimensionality problem.
Tensor methods are machine learning or dimensionality reduction techniques that
consider each tensor mode separately, reducing a given problem into partial,
per-mode problems.

Tensor methods often decompose tensors into a lower dimensional structure
of a core tensor and factor tensors.
The most common approaches adhere to either the Tucker structure or the PARAFAC
structure.
A Tucker decomposition reduces an input tensor of order $K$ with dimensions
$(D_1,D_2,\ldots,D_K)$ to a dense tensor with dimensions $(R_1,R_2,\ldots,R_K)$,
with $R_k \leq D_k$ for $k=1, 2, \ldots K$, using a set of per-mode factor
matrices.
Effective unsupervised tensor decomposition and approximation in the Tucker format can be achieved
using the \ac{hosvd}~\cite{DeLathauwer2000,SoleCasals2018}.
Alternatively, the \ac{parafac} structure can be used.
Here, the tensor is decomposed into a sum of rank-1 tensors, each the product
of a scalar and a vector per mode.
This is equivalent to a Tucker structured decomposition with all core elements
off the hyperdiagonal set to 0, as shown in \cref{fig:bttda/sparse}.
One way of obtaining an unsupervised PARAFAC decomposition is through the Canonical Polyadic
Decomposition~\cite{Hitchcock1927,Nazarpour2006}.
These decomposition methods can be regarded as feature extraction methods for a
\ac{bci} classification problem, with the flattened core tensors as feature vectors.
Extracted features can subsequently be classified to predict class labels, most
commonly using \ac{lda} or a \ac{svm}.


\begin{figure*}[t]
	\centering%
	\hfill%
	\input{figures/tensor_core_structures_1.tikz.tex}%
	\hfill\vrule\hfill%
	\input{figures/tensor_core_structures_2.tikz.tex}%
	\hfill%
	\caption[Core tensor(s) of different tensor decomposition structures.]{%
		A tensor decomposition finds core tensor and factor matrices
		from input tensor.
		This core tensor can have several structures.
		In the Tucker structure, the core is a dense tensor $\ten{G}$.
		The \ac{parafac} structure expresses the core as a sum of $B$ rank-1 terms, each
		with a scalar core $g^{(b)}$.
		The block-term structure expresses the core as a sum of $B$ smaller,
		Tucker-structured blocks $\ten{G}^{(b)}$.
		Both the \ac{parafac} and block-term structures are more sparse than the full
		Tucker structure, yet the block-term structure is more flexible as it
		allows blocks of variable tensor dimensionality instead of fixed rank-1 terms.
	}\label{fig:bttda/sparse}%
\end{figure*}


While commonly used, these Tucker or \ac{parafac} structures  might still not be able to
efficiently represent relevant neural information in a compressed format.
The block-term tensor structure is a generalization of both the Tucker and
\ac{parafac} structures.
It represents the tensor as a sum of Tucker structured terms.
If the number of terms is equal to 1, it is equivalent to the Tucker structure;
if the dimensions of each term are equal to 1, it is equivalent to the \ac{parafac}
structure.
The block-term structure (\cref{fig:bttda/sparse}, right) is more flexible than
either the Tucker or the \ac{parafac} structures, since it is not constrained to
solutions that must be expressed as either one of these structures and their
chosen hyperparameters.
Due to its flexibility, the block-term structure can strike a better
balance between extracting a maximal amount of relevant features and a minimal
amount of irrelevant features.
However, this increased flexibility comes at the cost of a higher number of
hyperparameters, as now both the number of terms and the dimension of
each term need to be specified.
A block-term structured core tensor can be obtained in an unsupervised way using
the \ac{btd}~\cite{DeLathauwer2008,DeLathauwer2008a,DeLathauwer2008b,Rontogiannis2021}.
Performance of methods leveraging either the Tucker and \ac{parafac} structures are
heavily dependent on the prior choice of hyperparameters describing
the desired reduced dimension or the number of rank-1 terms.

\subsection{Supervised tensor decompositions for \ac{bci}}

If the decompositions are not full rank, the Tucker, \ac{parafac} and block-term
structures are not unique and can be obtained by optimizing different criteria.
Given the low signal-to-noise ratio and specific, task-related output expected
in a \ac{bci} application, supervised feature extraction and machine learning techniques are
favored~\cite{Lotte2018} over the unsupervised decomposition methods presented
above.
A decomposition that is helpful for classification should ideally optimize
the discriminability between classes in the resulting core tensors, which can
then be considered as extracted features.
In this philosophy, the Tucker decomposition can also be obtained
using \ac{hoda}~\cite{Yan2005,Phan2010,Froelich2018}, which optimizes class
separability in the Fisher sense, analogous to linear discriminant analysis.

Variants of \ac{hoda} have been applied to \ac{bci} problems such as the decoding
of \acp{erp}~\cite{Onishi2012,Higashi2016} and \ac{mi}~\cite{Liu2015,Cai2021}
with positive results~\cite{Lotte2018}.
Recent work proposes optimization of the objective
function and introduces regularization~\cite{JamshidiIdaji2017,Jorajuria2022,Aghili2023}.
Discriminant tensor features have also been extracted
in the \ac{parafac} structure through manifold optimization~\cite{Froelich2018}.
However, it is not immediately obvious if either the Tucker or \ac{parafac}
structure are most suited to represent the neural data of interest for the
\ac{bci}
paradigm and for decoding.

More recent studies have shown that supervised decoders adopting a more flexible structure
can improve \ac{bci} performance.
Promising results have been achieved for regression tasks using
\ac{hopls}~\cite{Zhao2012,Camarrone2018} and \ac{bttr}~\cite{Faes2022,Faes2022a}.
\ac{bttr} has also been adapted into the classification variant \ac{bttc}~\cite{Camarrone2021}
but this methodology leaves room for improvement:
instead of optimizing features directly for class separability, \ac{bttc} regresses
to dummy 2-valued independent variable.
Thus, the method cannot be extended to a multi-class setting.
Furthermore, structures employed in these regression approaches could still
be considered as relatively constrained in comparison.
A more flexible approach could rely on a full block-term tensor decomposition
of the input data which optimizes discriminability and relies on a low-rank
common subspace between the input and classification labels.
\textcite{Huang2020} propose a supervised approach for finding multiple discriminant
multilinear spectral filter terms and apply it to motor imagery BCI, but their
decomposition is also limited in flexibility, since the solution is
restricted to terms with  dimension $(R_1,R_2,1)$, with mode 3
corresponding to the frequency domain.

\subsection{Contribution: A block-term structured model for classification}

With a proper choice of reduced dimension and number of terms, a
block-term decomposition directly optimizing discriminability might be more
suited to represent complex neural data in a sparse way, which additionally
yields a regularization effect.
Multiple parsimonious discriminant block terms with lower
dimensions might yield better performance than a single \ac{hoda} block
requiring a higher dimension to capture discriminant information, and by doing
so extracts too many irrelevant features.
A complementary view on the same approach goes as follows:
if HODA with a well-chosen reduced dimension extracts some discriminant features
from the input tensor, it is likely that it does not retrieve all useful
information due to the restrictions imposed by its Tucker structure.
Could \ac{hoda} therefore not sequentially be applied to extract discriminant
Tucker structured terms -- potentially with lower dimension -- as long as decoding
performance increases?

We implement this idea as a novel supervised feature
extraction method titled \ac{bttda}, a generalization of the aforementioned
\ac{hoda} algorithm.
\Ac{bttda} extracts discriminant features while adhering to a
flexible and efficient block-term tensor structure.
This work features the following contributions:
1) We develop a forward model for \ac{hoda} to reconstruct a given input tensor
from the extracted features.
2) This allows us to introduce \ac{bttda}\footnote{The source code of the
	methods introduced in this work is available at
	\url{https://github.com/arnevdk/bttda}.} as a state-of-the-art \ac{bci}
feature extraction method based on the block-term tensor structure.
3) We evaluate a \ac{bci} decoder based on \ac{bttda} and its special
\ac{parafac}-structured case on decoding benchmarks for both \ac{erp} and
\ac{mi} \ac{bci} paradigms and compare these to state-of-the-art decoders.

\section{Methods}

\subsection{Notation}
Tensors are indicated by bold underlined letters $\ten{X}$, matrices by bold
letters $\mat{U}$, fixed scalars by uppercase letters $K$, and variable
scalars as lowercase letters $k$.
The $n^\text{th}$ sample of a tensor dataset with $N$ samples is written as
$\ten{X}(n)$, the dataset itself as ${\{\ten{X}(n)\}}_n^N$.
A tensor $\smash{\ten{X}\in \mathbb{R}^{D_1\times D_2 \times \cdots \times D_K}}$ can be
unfolded in mode $k$ to a matrix
$\smash{\mat{X}_k\in\mathbb{R}^{(D_k\times\prod_{j\neq k}^K D_j)}}$, by concatenating
all mode $j\neq k$ fibers.
The tensor-matrix product of tensor $\ten{X}$ with matrix $\mat{U}$ along a
given mode $k$ is written as $\ten{X}\mpr{\mat{U}}{k}$. For ease of notation, let
$\ten{X}\mmpr{\mat{U}} =
	\ten{X}\mpr{\mat{U}}{1}\mpr{\mat{U}}{2}\cdots\mpr{\mat{U}}{K}$.
When skipping one of the modes $k$, this is
written as $\ten{X}\mmprs{\mat{U}}{k} =
	\ten{X}\mpr{\mat{U}}{1}\mpr{\mat{U}}{2}\cdots\mpr{\mat{U}}{k-1}\mpr{\mat{U}}{k+1}\ldots\mpr{\mat{U}}{K}$.
$\mat{A}\otimes\mat{B}$ indicates the Kronecker product of matrices $\mat{A}$
and $\mat{B}$.

\subsection{\Acl{hoda}}
\Acf{hoda}~\cite{Phan2010} is a
supervised, tensor-based dimensionality reduction and feature extraction technique.
For a set of $N$ tensors of order $K$
$\left\{\ten{X}(n)\in\mathbb{R}^{D_1\times D_2 \times \cdots \times
		D_K}\right\}_n^N$, HODA finds projection matrices $\mat{U_k}$ for each mode $k$
which project a given $\ten{X}$ to a latent tensor
$\ten{G}\in\mathbb{R}^{R_1\times R_2\times\cdots\times R_K}$, usually with lower
dimensions $(R_1\leq D_1,R_2\leq D_2,\ldots,R_K\leq D_K)$ using
tensor-matrix mode products:
\begin{equation}
	\ten{G}  = \ten{X}\mmpr{\mat{U}}
	\label{eq:HODA-backward}
\end{equation}
as visualized in \cref{fig:hoda} (left).
\begin{figure*}[t]
	\centering%
	\hfill%
	\input{figures/hoda_bw.tikz.tex}%
	\hfill\vrule\hfill%
	\input{figures/hoda_fw.tikz.tex}%
	\hfill%
	\caption[A \acs{hoda} backward projection.]{%
		Left: The multilinear projection obtained by \acf{hoda} applied to a third-order tensor
		sample $\ten{X}$ with dimensions $(D_1,D_2, D_3)$.
		\Ac{hoda} finds projection matrices $\mat{U}_k$ such that maximal
		discriminability between classes is achieved in the projected latent tensors
		$\ten{G}$ with reduced dimension $(R_1,R_2,R_3)$.
		Right: The forward projection for HODA.
		By calculating activation patterns $\mat{A}_k$, the original tensor $\ten{X}$ can approximately be
		reconstructed from projected latent tensor $\ten{G}$.
		The reconstruction is accurate up to an error term $\ten{E}$.
		$\mat{A}_k$ are chosen such that the variability captured in the latent tensor is
		maximally explained by the reconstructed tensor $\hat{\ten{X}}$ and not by
		the error term $\ten{E}$.
	}
	\label{fig:hoda}%
\end{figure*}

Since \ac{hoda} extracts latent features or properties $\ten{G}$ from the observed data
$\ten{X}$, relying on a task-related criterion, it can be referred to as a
\emph{backward model}.

Analogous to the \ac{hosvd}, \ac{hoda} decomposition results in a dense latent
tensor $\ten{G}$ and imposes an orthogonality constraint on each $\mat{U}_k$ to ensure uniqueness.
However, while \ac{hosvd} projection matrices minimize the reconstruction error,
\ac{hoda} optimizes the class discriminability of the reduced tensors
$\ten{G}(n)$ belonging to classes with labels $c_n$.
This is a desirable property in a classification setting where samples are
high-dimensional tensors.

\Ac{hoda} optimizes discriminability in the Fisher sense, maximizing the Fisher
ratio $\phi$ between the latent tensors $\ten{G}(n)$:
\begin{equation}
	\phi\left(\left\{\mat{U}\right\}\right) = \frac{\sum_c^CN_c\left\|\bar{\ten{G}}(c)-\bar{\bar{\ten{G}}}\right\|_F^2}
	{\sum_n^N\left\|\ten{G}(n)-\bar{\ten{G}}(c_n)\right\|_F^2}
	\label{eq:fisher}
\end{equation}
for $C$ classes with each $N_c$ samples. $\bar{\ten{G}}(c)$ is the mean of
latent tensors of class $c$, and $\bar{\bar{\ten{G}}}$ the mean of
these class mean latent tensors.
If the dimensions $(R_1,R_2, \ldots,R_k)$ are set a priori, the objective is now
to find the optimal projection matrices:
\begin{equation}
	\left\{\mat{U}^*\right\} =  \argmax_{\{\mat{U}\}}\phi\left(\left\{\mat{U}\right\}\right)
\end{equation}
which is solved through the backward HODA algorithm.
To start, $\mat{U}_k$ are initialized to orthogonal matrices, e.g.,\ as random
orthonormal matrices, by a per-mode \ac{svd},
or as the partial \ac{hosvd} of all stacked tensors in the dataset.
At each iteration, the algorithm loops through the modes and fixes all
projections but $\mat{U}_k$ corresponding to mode $k$.
It then finds a partial latent tensor:
\begin{equation}
	\ten{G}_{-k}=\ten{X}\mmprs{\mat{U}}{k}
\end{equation}
Subsequently, a new projection matrix $\mat{V}_k$ can be found analogous to Linear
Discriminant Analysis by constructing the partial within-class scatter matrix:
\begin{equation}
	\mat{S}_{-k,\text{w}} = \sum_n^N\tilde{\mat{G}}_{-k,k}(n)\cdot\tilde{\mat{G}}_{-k,k}^\intercal(n)
\end{equation}
with $\tilde{\ten{G}}_{-k}(n) = \ten{G}_{-k}(n) - \bar{\ten{G}}_{-k}(c_n)$,
and the partial between-class scatter matrix:
\begin{equation}
	\mat{S}_{-k,\text{b}} =
	\sum_c^CN_c\tilde{\bar{\mat{G}}}_{-k,k}(c)\cdot\tilde{\bar{\mat{G}}}_{-k,k}^\intercal(c)
\end{equation}
with $\tilde{\bar{\ten{G}}}_{-k}(c) = \bar{\ten{G}}_{-k}(c) - \bar{\bar{\ten{G}}}_{-k}$,
and solving for the $R_k$ leading eigenvectors in the eigenvalue problem:
\begin{equation}
	\mat{S}_{-k,\text{b}}-\varphi_k\mat{S}_{-k,\text{w}} =
	\mat{V}_k\mat{\Lambda}\mat{V}_k^\intercal
\end{equation}
with $\varphi_k=\tr\left(\mat{U}_k^\intercal\mat{S}_{-k,\text{b}}\mat{U}_k\right)/\tr\left(\mat{U}_k^\intercal\mat{S}_{-k,\text{w}}\mat{U}_k\right)$
using the $\mat{U}_k$ obtained in the previous iteration.
Finally, the orthogonal transformation invariant projections $\mat{U}_k$
are obtained by calculating the
per-mode total scatter matrices:
\begin{equation}
	\mat{S}_{k,\text{t}} = \sum_n^N\mat{X}_k(n)\cdot\mat{X}_k^\intercal(n)
\end{equation}
and finding the $R_k$ leading eigenvectors of:
\begin{equation}
	\mat{V}_k\mat{V}_k^\intercal\mat{S}_{k,\text{t}}\mat{V}_k\mat{V}_k^\intercal
	= \mat{U}_k\mat{\Lambda}\mat{U}_k^\intercal
\end{equation}
at each iteration~\cite{Wang2007}.
The iterative process halts when the
update of each $\mat{U}_k$ is lower than a predetermined threshold $\epsilon$ or after a
fixed number of iterations $I_\text{max}$.
The full \ac{hoda} procedure is summarized in \cref{alg:HODA}.
\begin{algorithm}
	\caption[A \acs{hoda} backward solution.]{The \acs{hoda} backward solution.}
	\label{alg:HODA}
	\input{algorithms/alg_hoda_bw.tex}
\end{algorithm}

To apply \ac{hoda} in a classification setting, the projections
are first learned on a training dataset with known class labels.
Next, these projections are used to extract latent tensors from the
tensors in the training dataset.
These latent training tensors are then reshaped (\emph{vectorized}) into feature vectors
$\mat{g} =  \vect(\ten{G})$ and used to train a decision classifier with the corresponding class labels.
At the evaluation stage, the projections learned from the training dataset are
used to extract latent tensors from an unseen test dataset with unknown class
labels, which can also be vectorized and passed on to the trained decision
classifier.

To avoid overfitting and improve performance in low sample size settings, the
HODA problem can be regularized by shrinking the partial
within-class scatter matrices~\cite{Phan2010} with a shrinkage factor
$\alpha_k$ at each step such that the eigenvalue problem becomes
\begin{equation}
	\mat{S}_b^{(-k)} -
	\varphi\left[\left(1-\alpha_k\right)\mat{S}_{-k,\text{w}}+\alpha_k\mat{I}\right] =
	\mat{V}_k\mat{\Lambda}\mat{V}_k^\intercal
\end{equation}
As in Linear Discriminant Analysis, the shrinkage parameter $\alpha_k$ can
also be estimated in a data-driven way in HODA~\cite{Jorajuria2022},
e.g., using the Ledoit-Wolf procedure~\cite{Ledoit2003} at every iteration.

\subsection{A forward model for \acs{hoda}}

As a prerequisite to the proposed \ac{bttda} model, we must find a
way to reconstruct the original data tensor $\ten{X}$ as accurately as possible
from $\ten{G}$ after dimensionality reduction.
This requires a \emph{forward} model, a generative model that expresses the observed data in
terms of given latent properties or features.
As indicated earlier, finding the optimal projection matrices ${U}$ that extract
tensors $\ten{G}$ given input data $\ten{X}$ as in \cref{eq:HODA-backward}
corresponds to fitting a backward \ac{hoda} model.
A forward model is a method to reconstruct the original data $\ten{X}$
from the core tensor $\ten{G}$.
Forward models are useful for, e.g., interpretability and data compression,
but here reconstruction with minimized reconstruction error is of interest.

A straightforward and computationally efficient candidate for the \ac{hoda}
forward model, visualized in \cref{fig:hoda} (right), is given as:
\begin{equation}
	\ten{X} = \ten{G}\mmpr{\mat{A}^\intercal} + \ten{E} =
	\hat{\ten{X}} + \ten{E}
	\label{eq:HODA-forward}
\end{equation}
with \emph{activation patterns} $\mat{A}_k \in \mathbb{R}^{D_k\times R_k}$,
reconstructed tensor $\hat{\ten{X}}$, and error term $\ten{E}$.

A good forward model should ensure that the norm of the reconstruction error
$\left\|\ten{E}\right\|_F$ is minimized.
In other words, variation captured in the latent tensor should be maximally captured by the
reconstruction term $\hat{\ten{X}}= \ten{G}\mmpr{\mat{A}^\intercal}$, and not by the error term
$\ten{E}$~\cite{Haufe2014}.
Hence, we aim to minimize the expected value of the cross-covariance between
the noise term and the extracted latent tensors:
\begin{equation}
	\left\{\mat{A}^*\right\}
	= \argmin_{\{\mat{A}\}}\text{E}\left[
		\text{vec}\left(\ten{E}(n)\right)\text{vec}\left(\ten{G}(n)\right)
		\right]_n
\end{equation}
or, equivalently~\cite{Parra2005,Haufe2014},
\begin{align}
	\left\{\mat{A}^*\right\}
	 & = \argmin_{\{\mat{A}\}}\sum_n^N\left[\ten{X}(n) -
	\hat{\ten{X}}(n)\right]^2                                                              \\
	 & = \argmin_{\{\mat{A}\}}\sum_n^N\left[\ten{X}(n) - \ten{G}(n)\mmpr{\mat{A}}\right]^2
\end{align}
This least-squares tensor approximation problem can be solved using the
Alternating Least Squares algorithm~\cite{Bentbib2022}, iteratively fixing all
but one of the activation patterns such that:
\begin{equation}
	\mat{A}_k = \argmin_{\mat{A}_k}
	\sum_n^N\left[\mat{X}_k(n) -
		\mat{A}_k\left(\ten{G}(n)\mmprs{\mat{A}}{k}\right)_k\right]^2
\end{equation}
at every iteration, which can be solved directly using ordinary least squares.
The activation patterns are initialized to the weights $\{\mat{U}\}$ of the
backward model.
Similar to fitting the backward model, the iterative process for the forward
model halts after a fixed number of iterations $I_\text{max}$ or when the update of each
$\mat{A}_k$ is lower than a predetermined threshold $\epsilon$.
The full procedure to determine the HODA forward projection is listed
in \cref{alg:HODA-fw}.
\begin{algorithm}
	\caption[A \acs{hoda} forward solution.]{The \acs{hoda} forward solution.}
	\label{alg:HODA-fw}
	\input{algorithms/alg_hoda_fw.tex}
\end{algorithm}

\subsection{\Acl{bttda}}
After defining the forward model, we can construct the proposed block-term
tensor model.
Assuming the latent tensors $\ten{G}$ obtained by the backward projection of
HODA do not achieve perfect
class separation, the error term $\ten{E}$ in \cref{eq:HODA-forward} contains
some discriminative information.
This, in turn, can be exploited to improve classifier
performance.
Useful features can then be extracted from $\ten{E} = \ten{X} -
	\hat{\ten{X}}$ by further projecting it onto another core tensor
$\ten{G}^{(2)}$, assuming $\ten{G}$ as $\ten{G}^{(1)}$.

We thus extend the \ac{hoda} feature extraction scheme to \acf{bttda}.
\Ac{bttda} finds multiple discriminative blocks, such that its forward
model adheres to the block-term tensor structure:
\begin{equation}
	\ten{X} = \sum_b^B\ten{G}^{(b)}\mmpr{\mat{A}^{(b)}} + \ten{E}
	\label{eq:BTTDA-forward}
\end{equation}
for $B$ extracted latent tensors $\ten{G}^{(b)}$ and residual error term
$\ten{E}$.
The \ac{bttda} model is further illustrated by~\cref{fig:BTTDA}.
\begin{figure*}[t]
	\centering
	\input{figures/bttda_fw.tikz.tex}
	\caption[A forward model for \acs{bttda}.]{A forward model for \acf{bttda}.
		\Ac{bttda} can extract more features
		than \ac{hoda} by iteratively finding a latent tensor $\ten{G}^{(b)}$ in a
		deflation scheme.
		The \ac{hoda} backward projection is first applied. Next, the
		input data is reconstructed via the HODA forward model and the
		difference between the two is found.
		Finally, this process is repeated with this difference as input data, until a
		desired number of blocks $B$ has been found.}
	\label{fig:BTTDA}
\end{figure*}
The block-term structure of this model implies that it is a generalization of both
the Tucker-structured \ac{hoda} and PARAFAC-structured discriminant feature
extraction.
If $B$ in \cref{eq:BTTDA-forward} is set to one, \ac{bttda} is equivalent to
\ac{hoda}; if at each term $b$ the dimension of the core tensor are
$(R_1^{(b)}=R_2^{(b)}=\ldots=R_k^{(b)}=1)$, a \ac{parafac} structure is assumed and
the resulting discriminant model is titled \ac{parafacda}.

Since \ac{bttda} is specified above as a forward model, a backward procedure
is required which finds the latent tensors $\ten{G}^{(b)}$ given $\ten{X}$ to
\ac{bttda} for feature extraction.
The extracted features represented by the latent tensors $\ten{G}^{(b)}$ can be
computed through a deflation scheme summarized in \cref{alg:BTTDA}.
\begin{algorithm}
	\caption{\Ac{bttda} feature extraction.}
	\label{alg:BTTDA}
	\input{algorithms/alg_bttda.tex}
\end{algorithm}
For each block $b$, the latent tensor is extracted using the HODA backward
projection from the residual error term of the previous
block $\ten{E}^{(b-1)}$ as in \cref{eq:HODA-backward}:
\begin{equation}
	\ten{G}^{(b)} = \ten{E}^{(b-1)}\mmpr{\mat{U}^{(b)}}
\end{equation}
This residual error term is calculated by finding the difference between the
previous error and its reconstruction after backward and forward \ac{hoda}
projection:
\begin{align}
	\ten{E}^{(b)}
	 & = \ten{E}^{(b-1)} - \hat{\ten{E}}^{(b-1)}                      \\
	 & = \ten{E}^{(b-1)} - \ten{G}^{(b)}\mmpr{\mat{A}^{\intercal(b)}}
\end{align}
with $\ten{E}^{(0)}=\ten{X}$.

The resulting latent tensors can be vectorized and concatenated into
one single feature vector per input tensor:
\begin{equation}
	\mat{g}
	=\left[\vect\left(\ten{G}^{(1)}\right)\
		\vect\left(\ten{G}^{(2)}\right)\
		\cdots\
		\vect\left(\ten{G}^{(B)}\right)\right]
\end{equation}
so that they can be classified in a similar manner to HODA.


\subsection{Model and feature selection}
Similar to the unsupervised \ac{btd}, the performance of
\ac{bttda} is heavily dependent on the number of blocks $B$ and their
corresponding dimensions $\{(R_1^{(b)}, R_2^{(b)}, \ldots,	R_K^{(b)})\}_b^B$.
If these are not known a priori or can not set based on insights into the
data generation process, a model selection step is necessary in order to
determine the optimal values for $R_k^{(b)}$ and $B$.
These hyperparameters can be set through cross-validated hyperparameter tuning,
although computationally expensive.

To reduce the hyperparameter search space, we introduce
a single hyperparameter $\smash{\theta \in [0,1]}$ which replaces the block
dimensions $\{(R_1^{(b)}, R_2^{(b)}, \ldots,	R_K^{(b)})\}_b^B$.
The new hyperparameter $\theta$ then controls the sparsity of the \ac{bttda} solution, with $\theta=0$
corresponding to the \ac{parafacda} model with blocks of dimension $(1,1,\ldots,1)$, and $\theta=1$
corresponding to blocks of full rank $(D_1, D_2,\ldots,D_K)$.
For $0 < \theta < 1$, the dimension of block $b$ can be determined
analogous to the method described by \textcite{Phan2010}.
Here, $R_k$ are chosen based on the number of components needed to explain a
certain proportion of the variability in a mode of the input data for a
Tucker-structured decomposition.
For the \ac{hoda} model used in \ac{bttda}, this can be achieved using the eigenvalues of the per-mode total scatter matrix of tensor $\ten{E}^{(b-1)}$
\begin{equation}
	\mat{S}_{k,\text{t}}^{(b)} = \sum_n^N\mat{E}_k^{(b-1)}(n)\cdot\mat{E}_k^{(b-1)\intercal}(n)
	= \mat{W}_k^{(b)}\mat{\Lambda}_k^{(b)}\mat{W}_k^{(b)\intercal}
\end{equation}
such that
\begin{equation}
	R_k^{(b)} = \argmin_{R\in 1,\ldots,D_k}\frac{\sum_r^R\lambda_{k,r}^{(b)}}{\sum_r^{D_k}\lambda_{k,r}^{(b)}} > \theta
\end{equation}

Finally, \ac{hoda}, and by extension \ac{bttda}, can extract a substantial amount
of redundant features.
These should be dropped after projection and before proceeding to the classification
step~\cite{Phan2010}.
In \ac{bttda} in particular, redundant features can accumulate over the number of
blocks, hampering performance.
Furthermore, discriminant features across blocks can be heavily correlated since
all blocks are independently optimizing the same discriminability criterion.

To tackle these issues, extracted features are first decorrelated and scaled using
a whitening \ac{pca} transformation, retaining all principal components.
Relevant \ac{pca} components can be identified by calculating the
univariate Fisher score $\phi(i)$ for each component $i$ after \ac{pca},
calculated as
\begin{equation}
	\phi(i) = \frac
	{\sum_c^C N_c \left[\bar{g}_i(c)-\bar{\bar{g}}_i\right]^2}
	{\sum_n^N \left[g_i(n)-\bar{g}_i(c_n)\right]^2}
\end{equation}
Only features where $\phi(i) > 1$, i.e., between-class variance is greater
than within-class variance, are retained.
If there  are no extracted features with $\phi(i) > 1$, only the feature with the highest
$\phi(i)$ is retained.

\section{Experiments}
\subsection{Datasets and decoders}
We evaluated our proposed model in two offline \ac{eeg}-based \ac{bci} decoding problems:
the \acf{erp} and \acf{mi} paradigms using the openly available \ac{moabb} datasets
(version 1.2.0)~\cite{Aristimunha2023}.
\Ac{moabb} is widely accepted as a suitable benchmark for decoders aimed at
classical \ac{bci} problems, allowing fair comparison of machine learning classifiers
independent from data preprocessing.
Details about these datasets are found in \cref{tab:moabb}.
The \ac{erp} decoding task focuses on distinguishing target from non-target \acp{erp},
while the \ac{mi} tasks consists of distinguishing different imagined or performed
limb movements.
Within-session classification performance was assessed using stratified 5-fold
cross-validation. Performance was calculated as the \ac{rocauc} for binary
classification problems and accuracy for multi-class problems, in line with
\ac{moabb} benchmarking framework.
Average performance scores are balanced over dataset by taking the mean of
the per-dataset average performance scores.

To use \ac{hoda}, \ac{bttda}, and \ac{parafacda} as a decoder, they are paired
with \ac{lda} to classify the extracted features (HODA+LDA).
Hyperparameters candidates $\theta \in \left\{0, 0.1, 0.2, \ldots 1\right\}$
for all three decoders and $b \in\left\{1,2\ldots,16\right\}$ in the case
PARAFACDA+LDA and BTTDA+LDA
were tuned each evaluation fold using nested, stratified 5-fold cross-validation.
Other \ac{hoda} hyperparameters were set to $\epsilon=\num{1e-6}$ and $I_\text{max}=128$.

Differences in classification score between these proposed decoders
were statistically verified using one-sided Wilcoxon rank-sum tests performed per
dataset and decoder
pair on the cross-validated scores per subject and session.
Following the \ac{moabb} evaluation framework, meta-analyses for all \ac{erp} and \ac{mi} datasets respectively
were performed using the Stouffer method and effect size was determined as the
\ac{smd} between classification scores.

As additional comparison with other commonly used decoders, we selected a subset
of the decoders evaluated by \textcite{Chevallier2024}.
These decoders have been thoroughly evaluated on the \ac{moabb} benchmark to
identify them as generally accepted state-of-the-art methods.
For the \ac{erp} task, these included the Riemannian Geometry-based
classifiers ERPCov+MDM, ERPCovSVD+MDM, XDAWNCov+MDM, XDAWNCov+TS+SVM and the linear
classifier.
For the \ac{mi} task, the comparison methods were selected from Riemannian
methods ACM+TS+SVM, FgMDM, TS+EL, and the deep learning classifiers EEGTCNet
and ShallowConvNet.
We refer to \textcite{Chevallier2024} for the description, implementation details
and references of these methods.

\subsection{Event-Related Potentials}
\Acp{erp} are spatiotemporal features, with each sample forming a
second-order tensor with $K=2$ modes (a matrix), representing \ac{eeg}
channels and time samples
per epoch.

The \ac{erp} datasets listed in \cref{tab:moabb}
are first processed according to the \ac{moabb} framework.
\Ac{eeg} signals were recorded at the sample rate given
by \cref{tab:moabb} and band-pass filtered between 1 Hz
and 24 Hz.
The signals were cut into epochs starting from stimulus onset with a
dataset-specific length given by \cref{tab:moabb}.
For HODA+LDA, PARAFACDA+LDA, and BTTDA+LDA decoders, epochs were further
downsampled to 48 Hz.

When considering grand average \ac{rocauc} over all evaluated \ac{erp} datasets
as reported in \cref{tab:results/erp/score},
the full BTTDA+LDA model (avg. \ac{rocauc}: 91.25$\pm$6.77\%) outperforms PARAFAC+LDA
(90.94$\pm$6.90\%),
and both in turn outperform HODA+LDA (88.89$\pm$7.04\%).
The meta-analysis shown in \cref{fig:results/meta} revealed the following
significant effects:
BTTDA+LDA > HODA+LDA ($p=\num{5.65e-65}$, SMD=$1.17$),
PARAFACDA+LDA > HODA+LDA ($p=\num{2.47e-58}$, SMD=$1.06$), and
BTTDA+LDA > PARAFAC+LDA ($p=\num{4.90e-15}$, SMD=$0.50$).
\begin{figure*}[t]
	\input{figures/pairwise_erp.tikz.tex}
	\vskip-1em

	\hskip-2em\input{figures/pairwise_mi.tikz.tex}
	\caption{%
		Meta-analysis of decoder classification performance comparisons per dataset.
		Analyses were performed on \ac{rocauc} score for \ac{erp} datasets (top) and
		accuracy for \ac{mi} datasets (bottom).
		For the evaluated \ac{erp} datasets, \ac{bttda} always outperforms \ac{hoda}.
		\Ac{bttda} outperforms \ac{hoda} for 3 out of 5 \ac{mi} datasets.
		$***$: $p<0.001$; $**$: $p<0.01$, $*$: $p<0.05$.
	}
	\label{fig:results/meta}
\end{figure*}
Both BTTDA+LDA and PARAFACDA+LDA always significantly outperform HODA+LDA.
BTTDA+LDA significantly outperforms PARAFACDA+LDA in 9 out of 14 datasets.
%Significance and effect sizes for all evaluated \ac{erp} datasets are reported in~\cref{tab:results/erp/stats}.

\begin{table*}
	\input{tables/score_erp.tex}
	\caption{Area under the receiver operating characteristic curve for
		cross-validated within-session evaluation of HODA+LDA and our proposed decoders
		PARAFACDA+LDA and BTTDA+LDA evaluated on \ac{erp} datasets.
		Scores for other decoders were taken from \textcite{Chevallier2024}.
		BTTDA+LDA always outperforms HODA+LDA and PARAFACDA+LDA, except for datasets,
		and consistently is nearly on par with or outperforms
		the state-of-the-art XDAWNCov+TS+SVM decoder.
	}
	\label{tab:results/erp/score}
\end{table*}
Compared to the state-of-the-art XDAWNCov+TS+SVM decoder, BTTDA+LDA scores
better in 8 out of 14 datasets, combined with a moderate increase in grand average \ac{rocauc}
($91.25 > 90.82$).


\subsection{Motor Imagery}

For \ac{mi}, discriminatory information is encoded in the \ac{eeg} data as
\acp{ersd}.
Contrary to the time-domain analyses performed on \acp{erp}, \acp{ersd} are
discerned in the power expressed in the time-frequency domain.
For the \ac{mi} task, we transform the \ac{eeg} signal into the
time-frequency domain, forming third-order tensors, with $K=3$ modes
respectively representing channels, frequencies, and time bins.

To achieve this, the \ac{eeg} signals in the \ac{mi} datasets listed in \cref{tab:moabb}
are first processed using the \ac{moabb} motor imagery pipeline.
\ac{eeg} signals were recorded at the sample rate given
by \cref{tab:moabb} and band-pass filtered between 8 Hz
and 32 Hz.
The signals were then cut into epochs starting from stimulus onset with a
dataset-specific length given by \cref{tab:moabb}.
Custom postprocessing to convert epochs to third-order tensors extracted
the magnitude of the complex Morlet-wavelet transform with 17 logarithmically spaced frequencies from 8 Hz to 32 Hz and a varying number of cycles logarithmically spaced from 4 to 16.
Finally, the magnitude envelope was downsampled to 32 Hz using an anti-aliasing
filter and decimation.

In line with the \ac{moabb} method, only the first three classes per dataset were
used.
When considering grand average classification accuracies over all evaluated
\ac{mi} datasets as reported in \cref{tab:mi-score},
the full BTTDA+LDA model (avg. accuracy: $64.52\pm12.23$\%)
outperforms PARAFACDA+LDA ($58.89\pm11.27$\%) and HODA+LDA
($61.00\pm11.11$\%).
The meta-analysis shown in \cref{fig:results/meta} revealed the following significant effects:
BTTDA+LDA > HODA+LDA ($p=\num{6.20e-5}$, SMD=$0.75$),
BTTDA+LDA > PARAFAC+LDA ($p=\num{4.00e-6}$, SMD=$1.48$).
BTTDA+LDA outperforms HODA+LDA except for datasets Zhou2016 and AlexandreMotorImagery.
PARAFACDA+LDA outperforms HODA+LDA for dataset Schirrmeister2017.
BTTDA+LDA outperforms PARAFACDA+LDA except for datasets Zhou2016 and AlexandreMotorImagery.

\begin{table*}
	\input{tables/score_mi.tex}
	\caption{Cross-validated classification accuracies for within-session evaluation
		to
		of HODA+LDA and our proposed decoders	PARAFACDA+LDA and BTTDA+LDA,
		evaluated on three-class motor imagery datasets.
		Tensor-based methods generally score lower than Riemannian Geometry-based
		decoders.
		\Ac{bttda} outperforms
		Accuracies for other decoders were taken from \textcite{Chevallier2024}.}%
	\label{tab:mi-score}%
\end{table*}
All of HODA+LDA (avg. accuracy $61.00\pm11.11$) and our proposed decoders PARAFACDA+LDA
($58.89\pm11.27$) and BTTDA+LDA ($64.52\pm12.23$) score
substantially lower than state-of-the-art decoder ACM+TS+SVM ($75.77\pm11.12$).

\subsection{Impact of block dimension and number of blocks}

To analyze the contribution of extra feature blocks extracted by BTTDA over
the first one found by HODA, we perform the following analyses on \ac{erp} dataset
BNCI2014-008 chosen for its minimal computational requirements.
We investigated cross-validated within-session \ac{rocauc} scores as function
of the number of blocks ($b$) and hyperparameter $\theta$, shown in \cref{fig:blocks} (left)
averaged over all subjects.
$b$ was varied from 1 to 16, while $\theta$ was chosen from
$\smash{\left\{0.0, 0.1, 0.2,\ldots, 1.0\right\}}$.
%Full results are presented in additional file \cref{item:add/blocks}.
Below, we report on selected of $\theta$ choices.
When $\theta=0$, the \ac{bttda} model corresponds to the \ac{parafacda} decoder.
$\theta=0.1$ yielded the highest BTTDA+LDA \ac{rocauc}.
$\theta=1$ resulted in the highest overall HODA+LDA ($b=1$) performance.
At $\theta=1$, no blocks other than the initial block can be modeled, since
$\theta=1$ by definition explains all data in the dataset and further forward
modeling fails.
\begin{figure}[t]
	\footnotesize
	\input{figures/gridsearch.tikz.tex}
	\caption{%
		Cross-validated BTTDA+LDA \ac{rocauc} (left) and \ac{bttda} \ac{nmse}
		(right) for dataset BNCI2014-008 as a function of the number of blocks $b$
		and the hyperparameter $\theta$	which controls the block dimensions.
		Class separation effectiveness increases as $b$ increases while \ac{nmse}
		decreases. Eventually, overfitting occurs and class separation performance
		drops or plateaus depending on the effectiveness of feature selection as
		shown here.
	}
	\label{fig:blocks}
\end{figure}

At $b=1$, corresponding to the \ac{hoda} model, sparse models with $\theta=0$ (avg.
\ac{rocauc} 83.16\%) and
$\theta=0.1$ (avg. \ac{rocauc} 83.05\%) are substantially lower than the
optimal performance at $\theta=1.0$ (avg. \ac{rocauc} 85.40).
Moving from the \ac{hoda} model ($b=1$) to the \ac{bttda} and \ac{parafacda}
model allows the extraction of more blocks ($b\geq1$).
With this relaxation, \ac{parafacda} and \ac{bttda} ($\theta=0.1$) exceed HODA
the at $b=3$ (avg. \ac{rocauc} 85.74\% and 85.71\% respectively), while
maintaining lower reduced dimensions than the (high) optimal dimensions for
\ac{hoda} ($\theta=1.0$).
Eventually, \ac{bttda} its reaches the highest overall \ac{rocauc} at $b=8$
(avg. \ac{rocauc} 86.23\%).
In general, when only a single block is used, a high $\theta$ is needed.
When more blocks are used, higher $\theta$ deteriorates performance.
Higher performance can be reached by choosing a low $\theta$ and $b>1$, resulting
in multiple blocks with low dimensions.

Additionally, \cref{fig:blocks} (right) shows the effectiveness of the forward
modeling step measured as the cross-validated \ac{nmse} when reconstructing
the original data  from the truncated \ac{bttda} decomposition
$\textstyle{\hat{\ten{X}}^{(B)}=\sum_b^B\ten{G}^{(b)}\mmpr{\mat{U}^{(b)}}}$.

\Ac{nmse} decreases monotonically with $b$ for both $\theta=0.0$ and $\theta=0.1$.
In general, \ac{nmse} decreases faster as $\theta$ increases.
For $\theta=1$, reconstruction \ac{nmse} at $b=0$ is near zero ($\num{1.32e-30}$)
since no information is lost in the full-rank decomposition.

\subsection{Interpretable decomposition}

The following qualitative analysis reveals the model interpretability provided
by the forward modeling step, by relating patterns in the reconstructed data
to expected effects visible in the neural data at hand.

All three proposed models were trained on the combined subjects in BNCI2014-008
for \ac{erp} classification and AlexMI for \ac{mi}.
To allow proper visual inspection, the \ac{erp} epochs were extended with
a pre-stimulus interval of 0.2 s for baseline correction and the original sample
rate of 256 Hz was kept.
The \ac{mi} epochs were sampled at 250 Hz after time-frequency transformation.
The number of blocks in this example was set to $B=2$ and hyperparameters $\theta$ were tuned
using 5-fold stratified cross-validation with entire-subject holdouts to
determine the best hyperparameter for cross-subject decoding
(\ac{erp}: $\theta=0.3$, \ac{mi}: $\theta=0.7$).
Each model was retrained with these hyperparameters on the full data combined
over all subjects.
Using these models,
These models then generated reconstructed contrasts
$\bar{\ten{C}}_{c_2-c_2}^{(b)}$ between classes $c_2$ and $c_1$ for $b=1$ and
$b=2$ as in \begin{equation}
	\bar{\ten{C}}_{c_2-c_1}^{(b)} = \left[
		\bar{\ten{G}}_{c_2}^{(b)}
		- \bar{\ten{G}}_{c_1}^{(b)}
		\right]\mmpr{\mat{A}^{(b)}}
\end{equation}
with $c_2$ target and $c_1$ non-target trials for \ac{erp}, and $c_2$ right hand
imagery and $c_1$ rest for \ac{mi}.
These contrasts, together with the grand-average contrast, are shown in
\cref{fig:results/interpret}
\begin{figure*}
	\input{figures/interpretability/erp_contrast.tikz.tex}

	\input{figures/interpretability/mi_contrast.tikz.tex}

	\caption{%
		Per-block forward \ac{bttda} model activation pattern contrasts and
		overall grand-average contrast for \ac{erp} dataset BNCI2014-008 (top) and
		\ac{mi} dataset AlexMI (bottom).
		Red lines indicate the slices generating the scalp plot.
		In the bottom row, the white dot indicates for which channel the time-frequency
		spectrum was plotted.
		The \ac{erp} is decomposed in parieto-occipital components (P1, N1, N2)
		corresponding to visual processing and fronto-central components (P3a,P3b)
		related to task processing.
		The right-hand \acf{mi} \ac{ersd} is decomposed in mostly contralateral high-$\mu$ band
		desynchronization, and parietal $\alpha$-band	synchronization.
	}
	\label{fig:results/interpret}
\end{figure*}

The grand-average \ac{erp} contrast shows an entangled superposition
of several different \ac{erp} components~\cite{Luck2011}.
The activation patterns of the first two blocks disentangle this contrast
in effects that can be related to \ac{erp} literature in the context of the
classic visual P300 matrix speller task in BNCI2014-008~\cite{Riccio2013}.

Block 1 exhibits positive and negative peaks in the lateral parieto-occipital
regions corresponding to the visual cortex.
The first positive peak and 2 negative peaks (P1, N1, N2)
correspond to early components reflecting the task-related visual processing
modulated by a mix of visual fixation and visual attention~\cite{Treder2010}.
Block 2 has a more central scalp expression, and shows 2 positive peaks (P3a, P3b).
Together with the residual positive activation between 0.4 s and 0.6 s in block
1, these constitute the processing of the attention-modulated detection of rare
stimuli present in the P300 matrix speller task~\cite{Kamp2013}.

For motor imagery, results are displayed in the time-frequency domain.
Positive values indicate event-related synchronization, negative values
desynchronization.
Upon visual inspection, the grand-average contrast shows no dominant pattern of
synchronization or desynchronization, possibly due to the limited dataset size.

\Ac{bttda} decomposition extracts two distinct effects.
Block 1 shows a persistent desynchronization between 9Hz and 13Hz most prominent
in the left central area.
For right-hand motor imagery, this corresponds to expected task-specific and
localized high-$\mu$ band desynchronization in the contralateral motor
cortex~\cite{Pfurtscheller2000,Wolpaw2012}.
Block 2 exhibits a synchronization between 8 and 12 Hz over the
parieto-occipital region, from 1.2 to 2.2 s.
This may be interpreted as the $\alpha$ band surround-ERS observed during hand
movement~\cite{Suffczynski1999, Gerloff1998,Wolpaw2012}.

\section{Discussion}
\subsection{Contribution}

The \ac{hoda} model used for \ac{bci} decoding can be constrained by its
Tucker structure.
We introduced a more flexible generalization termed \ac{bttda} with a
block-term tensor structure.
We also introduce \ac{parafacda}, a special case of \ac{bttda} expressed as a
sum of multilinear rank-1 terms.
Our results show that \ac{bttda} consistently scores on par or significantly higher than
\ac{hoda} as a supervised dimensionality reduction technique for \ac{bci} decoding.
\Ac{bttda} managed to outperform \ac{hoda} with 2.36\%pt. on average for
\ac{erp} datasets, and 2.75\%pt. for \ac{mi} datasets.
\Ac{parafacda} also scored 0.31\%pt. higher than  \ac{hoda} in \ac{erp} datasets
but was outperformed by \ac{bttda} overall.

BTTDA yields state-of-the-art decoding performance for \ac{erp}
datasets in the \ac{moabb} benchmark, but fails to do so for \ac{mi} datasets.
While this effect is not consistent over all \ac{erp} datasets, and the
increase is often rather low (<2\%pt.), it averages out over all datasets
as a moderate increase of 0.43\%pt.
We note that performances of other decoders for these problems already achieve
relatively high binary classification performance, which does not always leave
room for improvement.

As mentioned above, results for \ac{mi} were substantially lower than expected.
Not only does \ac{bttda} perform poorly, but the baseline \ac{hoda} model as well.
This is unexpected since it conflicts with literature which uses \ac{hoda} to
effectively classify \ac{mi} from time-frequency transforms~\cite{Phan2010,Lotte2018,Liu2015,Cai2021}.
If the issues hampering \ac{hoda} performance can be identified, \ac{bttda}
could gain ground on the state of the art.
We believe poor \ac{mi} performance in our case could stem from the following issues.
The time-frequency decomposition and data transformations or their parameters
used in this study might not be suited to capture the relevant \ac{ersd}
information necessary for performant classification.
For fair comparison with other MOABB decoders, the standard MOABB preprocessing
pipeline was followed, which might interfere with our postprocessing.
In this case, \ac{mi} decoding might benefit from different preprocessing,
transformation or	tensorization techniques.
On the other hand, hyperparameter selection could require more candidates or
cross-validation folds due to the combination of $K=3$ and larger data size following to the
time-frequency transformation.
Solutions for this problem can be computationally expensive.


Finally, due its the inherent forward modeling
steps, \ac{bttda} is intrinsically an explainable model which allows for interpretation
of the signal components modeled by the tensor blocks.
While the weights of the backward projections are
hard to interpret~\cite{Haufe2014}, the activation patterns and contrasts after
forward projection can reveal patterns in neural data.
Qualitative analyses showed that block activation patterns correspond to
task-related physiological processes for both \ac{erp} and \ac{mi}
classification problems.
Given informed or correctly tuned hyperparameters, this method could be used to,
e.g., separate and identify neural processes based on the task-related
information in the class labels.
More generally, \ac{bttda} can achieve an effective unmixing of signal generators
relevant to the classification problem at hand, which might otherwise not be
properly separated within the constraints of the \ac{hoda} model.
A point of care, however, arises from the deflation scheme: some processes
might already be partially explained by previous blocks.
In this case, information from a single physiologic process might not be modeled
using only a single block.
Hence, previous blocks might need to be taken into account to properly interpret
a block activation pattern.

\subsection{Modeling assumptions}

We assume the main benefit of \ac{bttda} stems from the following two aspects.
Given fixed block dimensions, extra \ac{bttda} blocks with proper feature selection
can discover more discriminant information over \ac{hoda}.
While no proof is given here, we show that NMSE monotonically decreases.
This suggests that all the variation in the signal will eventually be explained
by the model while still extracting features that are maximally discriminant.
Eventually, the number of blocks will reach a point of diminishing validation
score returns.
At this point, adding extra features to the decision classifier increases
the risk of overfitting instead of adding extra useful discriminatory
information.
Hence, performance increases with the number of blocks until overfitting occurs.

On the other hand, a \ac{bttda} solution is more parsimonious than a \ac{hoda}
solution can achieve due to its block-term structure compared to \ac{hoda}'s full Tucker
structure, as illustrated	by \cref{fig:bttda/sparse}.
In other words, the same discriminative information captured by a relatively large
Tucker-structured core tensor could be expressed more sparsely with a small
number of block-terms, while avoiding redundant features.
The \ac{parafac} structure employed in \ac{parafacda} is even more sparse, which could be
a benefit or a drawback depending on the amount of regularization required,
or on the true underlying structure of the data.
\Ac{bttda} with a few, sparse blocks might perform worse then a dense \ac{hoda}
solution, adding extra \ac{bttda} blocks eventually overpasses the \ac{hoda}
solution as indicated by~\cref{fig:blocks}.

The enhanced performance could also partially stem from \ac{bttda}'s internal
model of the data covariance.
Since HODA estimates one within-class scatter matrix
$\mat{S}_{-k,\text{w}}\in\mathbb{R}^{D_k\times D_k}$ per mode during training,
its overall model of the data scatter is determined by these per-mode scatter matrices as a
Kronecker product $\mat{S}_{-1,\text{w}}\otimes \mat{S}_{-2,\text{w}}\otimes\cdots\otimes \mat{S}_{-K,\text{w}}$.
This corresponds to the assumption that the \ac{eeg} data is
drawn from a multilinear normal distribution~\cite{Ohlson2013}.
Similar assumptions are made in \ac{erp} decoding algorithms such as
Spatial-Temporal Discriminant-Analysis~\cite{Zhang2013} and LCMV-beamforming
with Kronecker covariance structure~\cite{Kerchove2022}.
However, it is known that \ac{eeg} covariance cannot fully be expressed as a
single Kronecker product.
Rather, it is more accurately modeled as a sum of
multiple Kronecker products~\cite{Bijma2005, Sosulski2022}.
Since \ac{bttda} iteratively fits \ac{hoda} models to the residual error, each with its own
multilinear covariance model, it allows modeling multiple different multilinear
covariance terms, refining the internal covariance model.
This way, multiple effects with corresponding multilinear distributions
can be extracted.

Finally, the drop in performance for \ac{parafacda} in \ac{mi} datasets is
attributed to the \ac{parafac} interaction of the model's rank-1 term structure
with the multi-class nature of the \ac{mi} problems.
\Ac{parafacda} only extracts a single feature per block, which cannot properly
separate more than 2 classes.
Further blocks are not properly adapted to take into account which classes
have been separated by earlier blocks, hence extracting more \ac{parafac}
blocks might not be helpful.

In summary, we conclude there is an effective	added value in iteratively extracting
multiple block terms.
The flexibility of the \ac{bttda} model is both expressed in its ability
to capture more discriminant information with more parsimony,
and in its ability to capture effects which cannot be expressed by the \ac{hoda}
model, such as the \ac{eeg} covariance structure.
This makes it specifically suited to tackle classification problems encountered in
brain-computer interfacing.

\subsection{Model selection}

\Ac{bttda} trades the rigid \ac{hoda} model for increased model complexity with more
hyperparameters to tune, which expands the solution space to settings where
performance can be improved.
Extracting more blocks and tuning the hyperparameters increases the time
complexity of fitting \ac{bttda}-based models compared to \ac{hoda}-based models.
The most computationally expensive step in \ac{hoda} feature extraction is calculating
the multi-mode products $\ten{G} = \ten{X}\mpr{\mat{U}}{-k}\ \forall n$ with time
complexity $\mathcal{O}\left(NKD^{K+1}\right)$.
HODA then has complexity $\mathcal{O}\left(I_\text{max}NK^2D^{K+1}\right)$
when repeating this step over the outer loop with $I_\text{max}$ iterations and
the inner loop with $K$ iterations
In hyperparameter optimization, this gives complexity
\begin{equation}
	\mathcal{O}\left(\left|\Theta\right|FI_\text{max}NK^2D^{K+1}\right)
\end{equation}
with $\Theta$ the set of $\theta$ candidates and $F$ the number of
cross-validation folds for hyperparameter tuning.
\ac{bttda} training and model selection increases this to
\begin{equation}
	\mathcal{O}\left(\left|\Theta\right|FBI_\text{max}NK^2D^{K+1}\right)
\end{equation}
when using a grid search over $\theta$ combined with a line search over $b$.
Overall, the \ac{bttda} approach shifts the focus of tensor discriminant analysis
from finding optimal projections to model selection driven by computation.

The proposed $\theta$-controlled selection procedure efficiently
reduces the computational demand compared to tuning all hyperparameters
$\textstyle{\left\{  \left(R_1^{(b)},R_2^{(b)},\ldots,R_K^{(b)}\right)\right\}_b^B}$.
On the other hand, it also limits the chosen dimensions of each block to lie
within a subset of all possible configurations.
In a sense, this goes against the earlier proposition of increased model
flexibility.
Instances could occur where \ac{bttda} offers little to no added value over the
Tucker-structured \ac{hoda} when both are given totally free choice of
dimensions, but cases where \ac{bttda} could achieve greater performance could
equally be found.
Finding these optimal-dimension configurations, can currently only be achieved
through a costly, cross-validated hyperparameter search jointly over the
dimensions of each block.
Applications such as light-weight or mobile brain-computer interfaces should
carefully weight potential performance gains against this computational demand.
Future efforts should focus on more advanced automated hyperparameter selection
methods relying on sparsity criteria, eigenvalue truncation or information
criteria such as the ones used in \ac{bttr}~\cite{Faes2022}, or other
statistical measures depending on the application of the model.

Finally, we note that our proposed model selection procedure does not
guarantee grouping coherent projections within the same block according to some
desirable metrics.
Features across blocks are heavily correlated, leading to a high
degree of multicollinearity.
Currently, this is corrected  post-hoc by applying whitening and PCA.
Solutions imposing some sense of subspace orthogonality between the extracted
blocks could lead to a more effective feature extraction solution.
Sparsity, pattern interpretability, minimal or maximal within-block feature
correlation and ordering of blocks by decreasing discriminability are all
examples of useful within-block grouping criteria.


As future work, The impact of higher-order tensors ($K>3$) should be thoroughly
investigated, since this could have a large impact on model behavior.
We expect a dimensionality limit beyond which the forward modeling step cannot
accurately regress from the low-dimensional latent tensors to the
high-dimensional original tensors, introducing error in the input data for the
next block which can stack up over blocks.
The forward multilinear least squares problem is underdetermined hence prone to
numerical instability, which calls for a suited regularization approach.
Finally, other tensorization methods of the \ac{eeg} data should be explored,
such as time-lagged Hankel tensors~\cite{Papy2005} or tensors across subjects,
conditions or sliding windows if they are appropriate given the available prior
knowledge of the dataset.

\section{Conclusion}

We have introduced \acf{bttda}, a novel,
tensor-based, supervised dimensionality reduction technique optimized for class
discriminability, which adheres to the block-term tensor structure.
\Ac{bttda} is a generalization of \acf{hoda} and can also be
applied as a special sum-of-rank-one tensors \ac{parafacda} model.
The model is obtained by iteratively fitting \ac{hoda} in a deflation scheme,
leveraging a novel forward modeling step.

Via accompanying model selection hyperparameters, \ac{bci} decoders using
\ac{bttda} feature extraction can significantly outperform decoders based on
\ac{hoda} exceed state-of-the-art decoding performance on \acl{erp} problems
(second-order tensors) and outperform \ac{hoda} in motor imagery problems
(third-order tensors).
The inherent forward model of \ac{bttda} also allows interpreting the
discriminative processes considered by the classifier.

Moving from the rigid Tucker tensor structure of \ac{hoda} to the more flexible
and sparse block-term structure shifts the focus from finding the best constrained
multilinear projections to model and feature selection.
This approach allows performance and generalization to be traded for computational cost,
which is particularly relevant for \ac{bci} decoding problems.
Because of its general implementation and minimal assumptions on data structure,
\ac{bttda} can equally be applied to classification for other neuroimaging modalities
(MEG, ECoG, fNIRS, fMRI, EMG, etc.), or to tensor classification problems in other
domains.


\section*{Acknowledgment}
We thank the Flemish Supercomputer Center (VSC) and the High-Performance
Computing (HPC) center of KU Leuven for allowing us to execute our
computational experiments on their systems.
We also wish to acknowledge Dr.\ Axel Faes for his inspiration in
conceptualizing this work.



\printbibliography%

\appendix

\begin{table*}[h]
	\footnotesize
	\input{tables/moabb_datasets.tex}
	\caption{MOABB datasets used for evaluation, with the number of
		subjects (\# Sub.), the number of EEG channels (\# Chan.), the number of trials or trials per class for ERP
		datasets (\# Trials), the epoch length (Epoch len.), the sampling
		frequency (S. freq.), the number of sessions per subject (\# Sess.) and the
		number of runs (\# Runs). \ac{erp} datasets contain 2 classes, for \ac{mi} datasets the first 3 classes were retained. \Ac{erp} dataset Sosulski2019 was omitted due to technical problems.
		\Ac{mi} dataset PhysionetMI was omitted due to its high computational and
		storage demands.
		Adapted from~\cite{Aristimunha2023}
		and~\cite{Chevallier2024}.}%
	\label{tab:moabb}
\end{table*}

\end{document}
