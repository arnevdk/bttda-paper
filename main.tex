\documentclass[twocolumn]{article}

\usepackage[backend=biber]{biblatex}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{expl3}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{todonotes}

% Layout
\setuptodonotes{inline}


% Math declarations
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\newcommand{\ten}[1]{\underline{\mathbf{#1}}} % tensor
\newcommand{\mat}[1]{\mathbf{#1}} % matrix
\renewcommand{\vec}[1]{\mathbf{#1}} % matrix
\newcommand{\mpr}[2]{\times_{#2} {#1}_{#2}} % mode product
\newcommand{\mmpr}[1]{\times\{#1\}} % multi-mode product
\newcommand{\mmprs}[2]{\times_{-#2}\{#1\}} % multi-mode product
\newcommand{\ev}[2]{\text{E}\left[#1\right]_{#2}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newtheorem{theorem}{Theorem}

% Includes
\addbibresource{references.bib}
\input{includes/tensorviz.tex}

% Metadata
\title{Block-Term Tensor Discriminant Analysis for Brain-Computer Interfacing}

\begin{document}

\maketitle

\section{Introduction}

EEG/neuroimaging and tensor data

tucker

EEG methods:
Oscillatory source Tensor Discriminant Analysis
flavours of spatio-temporal discriminant analysis
Higher order spectral regression discriminant analysis
higher order linear feature learning

tucker, parafac, btd

bttr



Contribution:
* Forward model for hoda
* BTTDA and corresponding model selection scheme
* Evaluation of proposed and parafac on eeg benchmark
\section{Methods}

\subsection{Notation}
Tensors are indicated as bold underlined letters $\ten{X}$, matrices as bold
letters $\mat{U}$, fixed scalars as uppercase letters $K$ and variable
scalars as lower case letters $k$.
The $n^\text{th}$ sample of a tensor dataset with $N$ samples is written as
$\ten{X}(n)$.
A tensor $\ten{X}\in \mathbb{R}^{D_1\times D_2 \times \ldots \times D_K}$ can be unfolded in mode
$k$ to a matrix $\mat{X}_k\in\mathbb{R}^{(D_k\times\prod_{j\neq k}^K D_j)}$.
matrix
The tensor-matrix product of tensor $\ten{X}$ with matrix $\mat{U}$ along a
given mode $k$ is written as $\ten{X}\mpr{\mat{U}}{k}$. For ease of notation, let
$\ten{X}\mmpr{\mat{U}} =
	\ten{X}\mpr{\mat{U}}{1}\mpr{\mat{U}}{2}\cdots\mpr{\mat{U}}{K}$.
When skipping one of the modes $k$, this is
written as $\ten{X}\mmprs{\mat{U}}{k} =
	\ten{X}\mpr{\mat{U}}{1}\mpr{\mat{U}}{2}\cdots\mpr{\mat{U}}{k-1}\mpr{\mat{U}}{k+1}\ldots\mpr{\mat{U}}{K}$.
The Kronecker product is noted as $\otimes$, covariance matrices are indicated  with $\mat{\Sigma}$.

\subsection{Higher-Order Discriminant Analysis (\textsc{hoda})}
Higher Order Discriminant Analysis (\textsc{hoda})~\cite{Phan2010} is a tensor
feature extraction technique. For a $K^{th}$ order input tensor $\ten{X}$ of
shape $(D_1,D_2,\ldots,D_K)$, \textsc{hoda} finds projection matrices $\mat{U_k}$ for each mode $k$
that project $\ten{X}$ a core tensor $\ten{G}$, usually with lower
dimensionality $(r_1,r_2,\ldots,r_K)$ using tensor-matrix mode products
\begin{equation}
	\ten{G}  = \ten{X}\mmpr{\mat{U}}
	\label{eq:hoda-backward}
\end{equation}
visualized in Figure~\ref{fig:hoda-backward}.
\begin{figure}
	\centering
	\input{figures/hoda_backward.tikz.tex}
	\caption{A visualization of the multilinear projection learnt by Higher Order
		Discriminant Analysis (\textsc{hoda}) for a dataset of $N$ second order tensors
		$\ten{X}$ of shape $(D_1,D_2)$.
		\textsc{hoda} finds projection matrices $\mat{U}_k$ such that maximal
		discriminability between classes is achieved in the projected core tensors
		$\ten{G}$ with reduced dimensionality $(r_1,r_2)$.}
	\label{fig:hoda-backward}
\end{figure}
Analogous to the \textsc{tucker} decomposition, \textsc{hoda} is a dimensionality
reduction decomposition that results in a dense core tensor $\ten{G}$, and
imposes an orthogonality constraint on $\mat{U}_k$ to ensure uniqueness.
However, while for the \textsc{tucker} decomposition the projection matrices
are chosen to minimize the reconstruction error, the projection matrices
$\mat{U}_k$ of \textsc{hoda} are optimized for maximal discriminability between
$\ten{G}(n)$ belonging corresponding classes with labels $c_n$.
This is a desirable property in a classification setting where samples are
high-dimensional tensors.

\textsc{hoda} optimizes discriminability in the Fisher sense, by optimizing the
Fisher ratio between $\phi$ the core tensors $\ten{G}(n)$
\begin{equation}
	\phi = \argmax_{\mat{U}_k}
	\frac{\sum_c^CN_c\left\lVert\bar{\ten{G}}(c)-\bar{\bar{\ten{G}}}\right\rVert_F^2}
	{\sum_n^N\left\lVert\ten{G}(n)-\bar{\ten{G}}(c_n)\right\rVert_F^2}
	\label{eq:fisher}
\end{equation}
for $C$ classes with each $N_c$ samples. $\bar{\ten{G}}(c)$ is the mean of core
tensors $\ten{G}(n)$ of class $c$, and $\bar{\bar{\ten{G}}}$ the mean of
these class mean core tensors.
Equation~\ref{eq:fisher} can be solved by the iterative algorithm in
Algorithm~\ref{alg:hoda}.
To start, $\mat{U}_k$ are initialized to orthogonal matrices, e.g. as random
orthogonal matrices, by a per mode Singular Value Decomposition (\textsc{svd})
or by a Multi-Linear Singular Value Decomposition (\textsc{mlsvd})~\cite{Lathauwer2000} of the input data.
At each iteration, the algorithm loops trough the modes and fixes all the
projection of all but $\mat{U}_k$ corresponding to mode $k$.
It then finds a partial core tensor $\ten{G}_{-k}=\ten{X}\mmprs{\mat{U}}{k}$
Subsequently, a new projection matrix $U_k$ can be found analogous to Linear
Discriminant Analysis by constructing the within- and between-class scatter
matrices $\mat{S}_{-k,\text{w}}$ and $\mat{S}_{-k,\text{b}}$ of
$\mat{G}_{-k,k}$ the partial core tensor unfolded towards mode $k$, and solving
the generalized eigenvalue problem
\begin{equation}
  \mat{S}_{-k,\text{b}}\mat{U}_k = \mat{\lambda}\mat{S}_{-k,\text{w}}\mat{U}_k
\end{equation}

To apply \textsc{hoda} in a classification setting, the projections $\mat{U}_k$
can first be learned on a training dataset
$\left\{\ten{X}^\text{train}(m)\right\}_m^M$ with known class labels, and
training features $\left\{\ten{G}^\text{train}(m)\right\}_m^M$ can be extracted.
Next, these training features are reshaped (vectorized) into vectors
$\left\{\vec{g}^\text{train}(m) \in \mathbb{R}^{\prod_k^Kr_k}\right\}_m^M$ and a classifier is
trained on these features and the corresponding class labels.
Finally, the learnt projections $\mat{U}_k$ can be applied to unseen testing
dataset $\left\{\ten{X}(n)^\text{test}\right\}_n^N$ and the exctracted features
$\left\{\ten{G}^\text{test}(n)\right\}_n^N$ vectorized into
$\left\{\vec{g}^\text{train}(n)\right\}_n^N$ can be
classified by the trained classifier.\todo{remove set notations for brevity}
To avoid overfitting and improve performance in low sample size settings, the
\textsc{HODA} problem can be regularized by shrinking the the partial
whithin-class scatter matrix~\cite{Phan2010} with a shrinkage factor
$\alpha_k$ at each step such that the generalized eigenvalue problem becomes
\begin{equation}
	\mat{S}_b^{(-k)}\mat{U}_k =
	\mat{\lambda}((1-\alpha_k)\mat{S}_w^{(-k)}-\alpha_k\mat{I})\mat{U}_k
\end{equation}
Like in Linear Discriminant Analysis, the shrinkage parameter $\alpha_k$ can
also be estimated in a data driven way in \textsc{hoda}~\cite{Jorajuria2022},
e.g., using the Ledoit-Wolf procedure~\cite{Ledoit2003}.


\subsection{A forward model for \textsc{hoda}}
As a prerequisite for our proposed \textsc{bttda} model, we must first find a
way to reconstruct the original data tensor $\ten{X}$ as accurately as possible
from $\ten{G}$ after dimensionality reduction.
In neuroimaging, this is referred to as a \textit{forward model}.
While a \textit{backward model} extracts latent sources or properties from the observed
data, which can be optimised for tasks like regression or classification or
informed by prior knowledge about source propagation, a \textit{forward model} is a
generative model that expresses the observed data in function of some latent
properties or sources that are given.

The \textsc{hoda} projection in Equation~\ref{eq:hoda-backward} is an example
of a backward model.
A straightforward and computationally efficient candidate for a corresponding
forward model is
\begin{equation}
	\ten{X} = \ten{G}\mmpr{\mat{A}^\intercal} + \ten{\mathbfcal{E}}
	\label{eq:hoda-forward}
\end{equation}
with \textit{activation patterns} $\mat{A}_k \in \mathbb{R}^{D_k\times r_k}$
and unkown error term $\ten{\mathbfcal{E}}$.
\begin{figure}
	\centering
	\input{figures/hoda_forward.tikz.tex}
	\caption{The forward projection for \textsc{hoda}. Leveraging activation
		patterns $A_k$, the original tensor $\ten{X}$ can approximately be
		reconstructed from projected core tensor of latent factors $\ten{G}$. $A_k$ are chosen such
		that the variability captured in the latent factors is maximally expressed in
		the reconstructed tensor and not in the error term.}
	\label{fig:hoda-forward}
\end{figure}

A forward model should make sure that reconstruction error is minimized and
variation captured in the latent factors is maximally captured by the forward
projection term $\ten{G}\mmpr{\mat{A}^\intercal}$, and not by the error term
$\ten{\mathbfcal{E}}$~\cite{Haufe2014}.
This gives rise to the following calculation for activation patterns
$\mat{A}_k$:
\begin{theorem}
	The activation patterns $\mat{A_k}$ for the forward model in
	Equation~\ref{eq:hoda-forward} can be calculated as
	\begin{equation}
    \mat{A}_k =
    \mat{\Sigma}_{\mat{X}_k}\mat{U}_k\mat{\Sigma}_{\mat{X}_k\mat{U}}
		\label{eq:ap}
	\end{equation}
	\label{the:ap}
\end{theorem}
With $\mat{\Sigma}_{\mat{X}_k}$ the covariance matrix of $\mat{X}_k$ tensors $\ten{X}(n)$
unfolded towards mode $k$, and $\mat{\Sigma_{X_kU}}$  the covariance matrix of
$\mat{X}_k\mat{U}$.

\subsection{Block-Term Tensor Discriminant Analysis (\textsc{bttda})}
After defining the forward model, we can construct our proposed block-term
tensor model. Assuming the core tensors $\ten{G}$
obtained by the backward projection of \textsc{hoda} do not achieve perfect
class separation, the error term $\ten{\mathbfcal{E}}$ in
Equation~\ref{eq:hoda-forward} should still contain some discriminative
information, which can be exploited to improve classifier performance.
We thus extend the \textsc{hoda} feature extraction scheme with backward an
forward models defined in respectively Equations~\ref{eq:hoda-backward}
and~\ref{eq:hoda-forward} to Block-Term Tensor Discriminant Analysis
(\textsc{bttda}), with
a forward model given by
\begin{align}
  \ten{X} & = \ten{G}^{(1)}\mmpr{\mat{U}^{(1)}} +
  \ten{G}^{(2)}\mmpr{\mat{U}^{(2)}} \\
          &\quad + \cdots + \ten{G}^{(B)}\mmpr{\mat{U}^{(B)}} + \ten{\mathbfcal{E}}
  \label{eq:bttda-forward}
\end{align}
which extracts $B$ core tensors $\ten{G}^{(b)}$ from input tensor $\ten{X}$
until error term $\ten{\mathbfcal{E}}$ remains.
Figure~\ref{fig:bttda} further illustrates the \textsc{bttda} model.
\begin{figure*}
	\centering
	\input{figures/bttda.tikz.tex}
  \caption{A forward model for Block-Term Tensor Discriminant Analysis
  (\textsc{bttda}). \textsc{bttda} can extract more features
 than \textsc{hoda} by iteratively finding a core tensor $\ten{G}^{(b)}$ in a
 deflation scheme.
 The \textsc{hoda} backward model is first applied. Next, the reconstructed
 input data is reconstructed via the \textsc{hoda} forward model and the
 difference between the two is found.
 Finally, this process is repeated with this difference as input data, until a
 desired number of blocks $B$ has been found.}
  \label{fig:bttda}
\end{figure*}

Since the \textsc{bttda} is specified as a forward model, a procedure is
required that finds the core tensors $\ten{G}^(b)$ from $\ten{X}$ for
\textsc{bttda} to be useful as a feature extraction method.
The extracted features represented by the core tensors $\ten{G}^{(b)}$ can be
computed through a deflation scheme summarized in Algorithm~\ref{alg:bttda}.
The first block $\ten{G}^{(1)}$ can be obtained by applying
Equation~\ref{eq:hoda-backward} to $\ten{X}$.
\begin{equation}
  \ten{G}^{(1)} = \ten{X}\mmpr{\mat{U}^{(1)}}
\end{equation}
For the next blocks $\ten{G}^{(b)}~\forall~ b\in2,3,\ldots,B$, we first compute
the residual error after applying the \textsc{hoda} backward and forward
projection.
\begin{equation}
  \ten{\mathbfcal{E}}^{(b)} =
  \ten{G}^{(b)}\mmpr{\mat{U}^{(b)}}\mmpr{\mat{A}^{\intercal(b)}}
\end{equation}
The next block of features can then be obtained by calculating the
\textsc{hoda} backward projection on this residual error as
\begin{equation}
  \ten{G}^{(b+1)} = \ten{\mathbfcal{E}}^{(b)}\mmpr{\mat{U}^{(b+1)}}
\end{equation}

The resulting extracted feature tensors can be flattened and concatenated into
one feature vector, so that they can be classified in a similar manner to
\textsc{hoda}.


\subsection{Model and feature selection}
Similar to the unsupervised \textsc{btd}, the performance of \textsc{bttda} is
heavily dependent on the shape $\left(r_1^{(b)}, r_2^{(b)}, \ldots,
r_K^{(b)}\right)$ and on the number of blocks $B$.
If these are not kown a priori, i.e. if they cannot be set base on insights in the
data generation process, a model selection step is necessary in order to
determine what the optimal values for $r_k^{(b)}$ and $B$ are.
Hyperparameter tuning through cross-validation is straight-forward solution to
determine these parameters, yet can be computationally expensive.
To reduce the computational cost, Algorithm~\ref{alg:model-selection} propose a model selection algorithm that
leverages cross-validation in a greedy way per block, to iteratively find the
optimal size for the next block given the previous blocks.
\todo{Algorithm}
\todo{How to determine $B$}

\textsc{hoda} can extract a substantial amount of redundant features, that can
be dropped after projection and before proceeding to the classification
step~\cite{Phan2010}.
Since \textsc{bttda} extracts a set of \textsc{hoda}
feature tensors, these redundant features can stack up over the number of
blocks.
Therefore, we apply feature selection after finding all \textsc{bttda} blocks
by calculating the univariate Fisher score of the concatenated vectorized
features, and retain only those features that have  statistical significant
contribution to class separation with $p<\alpha$.
Since the extracted features are already optimized for class separation,
$\alpha$ should not be chosen too low.
The significance thershold in this work is set to $\alpha=0.95$, for all
evaluations of both \textsc{hoda} and \textsc{bttda}.


\section{Experiments}
\subsection{Datasets}
We evaluate our proposed model in multiple off-line BCI decoding setting.
MOABB~\cite{Aristimunha2023} is an openly available BCI benchmarking platform that contains
multiple EEG datasets for different BCI tasks.
Specifically, we will test our decoder on Event-Related Potential (ERP) tasks
such as the P300 paradigm, and on Motor Imagery (MI) tasks.
ERPs are spatiotemporal features, with each sample forming a $2^\text{nd}$
order tensor with 2 modes representing the channels and time samples.
For MI, discriminatory information is represented in the EEG data as
Event-Related Synchronizations/Desynchronizations (ERS/Ds).
Contrary the the time domain analysis performed on ERPs, ERS/Ds are only
detectable in the time-frequency domain.
Hence, for the MI task, we will transform the EEG signal into the
time-frequency domain, forming $3^\text{d}$ order tensors, with modes
representing the channels, the frequencies and time samplies and time samples.

Decrease of MSE as f ratio increases
\subsection{Order-2 tensors: event-related potentials}


\subsection{Order-3 tensors: motor imagery}
\section{Discussion}


%https://sci-hub.ru/https://ieeexplore.ieee.org/abstract/document/6287946
%https://www.sciencedirect.com/science/article/abs/pii/S0031320317301875
\printbibliography

\appendix
\section{Proof of Theorem~\ref{the:ap}}

Equations~\ref{eq:hoda-backward} and~\ref{eq:hoda-forward} can be expressed in fuction
of sample $n$ as
\begin{subequations}
	\label{eq:proj-n}
	\begin{align}
		\ten{G}(n) & = \ten{X}(n)\mmpr{\mat{U}}
		\label{eq:proj-back-n}                                                   \\
		\ten{X}(n) & = \ten{G}(n)\mmpr{\mat{A}^\intercal}+\ten{\mathbfcal{E}}(n)
		\label{eq:proj-fwd-n}
	\end{align}
\end{subequations}
%We write Equations~\ref{eq:proj-back-n} and~\ref{eq:proj-fwd-n} respectively
%as their unfolded multi-mode products for mode $k$
%\begin{subequations}
%	\label{eq:proj-unfold}
%	\begin{align}
%		%https://www5.in.tum.de/persons/huckle/tensor-kurs_1.pdf
%		\mat{G}_k(n) & =
%		\mat{U}_n\mat{X}_n(n)\left(\mat{U}_1\otimes\mat{U}_2\otimes\cdots\otimes\mat{U}_{k-1}\otimes\mat{U}_{k+1}\otimes\cdots\otimes\mat{U}_K\right)
%		\label{eq:proj-back-unfold} \\
%		\mat{X}_k(n) & =
%		\mat{A}_n^\intercal\mat{G}_n(n)\left(\mat{A}^\intercal_1\otimes\mat{A}^\intercal_2\otimes\cdots\otimes\mat{A}^\intercal_{k-1}\otimes\mat{A}^\intercal_{k+1}\otimes\cdots\otimes\mat{A}^\intercal_K\right)
%		+ \mat{\mathbfcal{E}}_k(n)
%		\label{eq:proj-fwd-unfold}
%	\end{align}
%\end{subequations}
%According to Theorem 1 in~\cite{Haufe2014}, the activation patterns defining the
%unfolded forward model in Equation~\ref{eq:proj-fwd-unfold} corresponding to the
%unfolded backward model in Equation~\ref{eq:proj-back-unfold} is given by
%\begin{align}
%	\mat{A}_k = \mat{\Sigma}_{\mat{X}_k}\mat{U}_k
%\end{align}
Let us now express these backward and forward models in their vectorized forms:
\begin{subequations}
	\label{eq:proj-n}
	\begin{align}
		\vec{g}(n) & = \vec{x}(n)\left(\bigotimes_k^K\mat{U}_k\right)
		\label{eq:proj-back-vec}                                                  \\
		\vec{x}(n) & = \vec{g}(n)\left(\bigotimes_k^K\mat{A}_k^\intercal\right) +
		\vec{\boldsymbol\epsilon}(n)
		\label{eq:proj-fwd-vec}
	\end{align}
\end{subequations}
Since $\bigotimes_k^K\mat{U}_k$ is a Kronecker product of orthogonal matrices,
which itself is orthogonal, the activation pattern
$\mat{B}\in\mathbb{R}^{\prod_k^KD_k\times\prod_k^Kr_k} $ of a vectorized forward model
corresponding to Equation~\ref{eq:proj-back-vec} is given according
to~\cite{Haufe2014} as
\begin{align*}
	\mat{B} & =
	\mat{\Sigma}_\vec{x}\left(\bigotimes_k^K\mat{U}_k\right)\mat{\Sigma}_\vec{g}^{-1} \\
	        & =
	\mat{\Sigma}_\vec{x}\left(\bigotimes_k^K\mat{U}_k\right)\left[\left(\bigotimes_k^K\mat{U}_k\right)^\intercal\mat{\Sigma_\vec{x}}\left(\bigotimes_k^K\mat{U}_k\right)\right]^{-1}
\end{align*}

Because \textsc{hoda} assumes the data covariance can be expressed as a
Kronecker product of mode-$k$ covariances
$\mat{\Sigma}_{\mat{X}_k}$\todo{citation  needed}, we get
\begin{align*}
	\mat{B} & =
	\left(\bigotimes_k^K\mat{\Sigma}_{\mat{X}_k}\right)\left(\bigotimes_k^K\mat{U}_k\right)
  \\
          & \quad \cdot
          \left[\left(\bigotimes_k^K\mat{U}_k\right)^\intercal\left(\bigotimes_k^K\mat{\Sigma}_{\mat{X}_k}\right)\left(\bigotimes_k^K\mat{U}_k\right)\right]^{-1}
	\\
	        &
	=\left(\bigotimes_k^K\mat{\Sigma}_{\mat{X}_k}\right)\left(\bigotimes_k^K\mat{U}_k\right)\left(\bigotimes_k^K\mat{U}_k^\intercal\mat{\Sigma}_{\mat{X}_k}\mat{U}_k\right)^{-1} \\
	        &
	=\left(\bigotimes_k^K\mat{\Sigma}_{\mat{X}_k}\right)\left(\bigotimes_k^K\mat{U}_k\right)\left[\bigotimes_k^K\left(\mat{U}_k^\intercal\mat{\Sigma}_{\mat{X}_k}\mat{U}_k\right)^{-1}\right]
	\\
	        & = \bigotimes_k^K \mat{\Sigma}_{\mat{X}_k}
	\mat{U}_k\left(\mat{U}_k^\intercal\mat{\Sigma}_{\mat{X}_k}\mat{U}_k\right)^{-1}
\end{align*}
and finally
\begin{align*}
  \mat{A}_k = \mat{\Sigma}_{\mat{X}_k}
	\mat{U}_k\left(\mat{U}_k^\intercal\mat{\Sigma}_{\mat{X}_k}\mat{U}_k\right)^{-1}
\qquad\blacksquare
\end{align*}



%Our proof follows the structure laid out in \cite{Haufe2014} (Appendix A).
%In the general case when $r_k<D_k$, $\mat{U}_k$ are not square and hence non-invertible.
%The backward and forward projections given by
%Equations~\ref{eq:hoda-backward} and~\ref{eq:hoda-forward} can be expressed in fuction
%of sample $n$ as
%\begin{equation}
%	\ten{G}(n) = \ten{X}(n)\mmpr{\mat{U}}
%	\label{eq:proj-back-n}
%\end{equation}
%and
%\begin{equation}
%	\ten{X}(n) = \ten{G}(n)\mmpr{\mat{A}^\intercal}+\ten{\mathbfcal{E}}(n)
%	\label{eq:proj-fwd-n}
%\end{equation}
%respectively, with unkown error term $\ten{\mathbfcal{E}}(n)$.
%
%Plugging the forward projection in Equation~\ref{eq:proj-fwd-n} into the backward projection in
%Equation~\ref{eq:proj-back-n} gives
%\begin{align*}
%	\ten{G}(n) & = \ten{X}(n)\mmpr{\mat{U}}                           \\
%	           & = (\ten{G}(n)\mmpr{\mat{A}^\intercal} +
%	\ten{\mathbfcal{E}}(n))\mmpr{\mat{U}}                             \\
%	           & = \ten{G}(n)\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}} +
%	\ten{\mathbfcal{E}}(n)\mmpr{\mat{U}}
%\end{align*}
%Taking the tensor outer product on the left with $\ten{G}$ yields
%\begin{align*}
%	\ten{G}(n)\otimes\ten{G}(n) & = \ten{G}(n)\otimes\ten{G}(n)\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}} \\
%	                            & \quad +	\ten{G}(n)\otimes\ten{\mathbfcal{E}}(n)\mmpr{\mat{U}}
%\end{align*}
%and when taking the expected value over samples
%\begin{align*}
%	 & \ev{\ten{G}(n)\otimes\ten{G}(n)}{n}                                                \\
%	 & \quad = \ev{\ten{G}(n)\otimes\ten{G}(n)\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}}}{n} \\
%	 & \quad\quad + \ev{\ten{G}(n)\otimes\ten{\mathbfcal{E}}(n)\mmpr{\mat{U}}}{n}       \\
%	 & \quad = \ev{\ten{G}(n)\otimes\ten{G}(n)}{n}\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}} \\
%	 & \quad\quad +  \ev{\ten{G}(n)\otimes\ten{\mathbfcal{E}}(n)}{n}\mmpr{\mat{U}}
%\end{align*}
%To find a forward projection that is corresponding to the backward projection,
%ensuring any variation explained by the projections is maximally captured in the
%activation patterns, we assume
%\begin{equation}
%	\ev{\ten{G}(n)\otimes\ten{\mathbfcal{E}}(n)}{n} = 0
%	\label{eq:uncorr}
%\end{equation}
%yielding
%\begin{align*}
%	 & \ev{\ten{G}(n)\otimes\ten{G}(n)}{n}                                                \\
%	 & \quad = \ev{\ten{G}(n)\otimes\ten{G}(n)}{n}\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}}
%\end{align*}
%Since the covariance tensor of the latent tensor,
%$\ev{\ten{G}(n)\otimes\ten{G}(n)}{n}$,
%has full tensor rank due to the linear independance of columns in projection
%matrices $\mat{U}_k$, and it exists in $\mathbb{R}^{r_1\times r_1\times
%		r_2\times \ldots\times r_K\times r_1\times r_2\times \ldots\times r_K}$, its tensor inverse exists and we derive
%\begin{equation}
%	\ten{I} = \ten{I}\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}}
%	\label{eq:identity}
%\end{equation}
%
%Plugging in the backward projection in Equation~\ref{eq:proj-back-n} into the
%forward projection in Equation~\ref{eq:proj-fwd-n} gives
%\begin{align*}
%	\ten{X}(n) & = \ten{G}(n)\mmpr{\mat{A^\intercal}}+\ten{\mathbfcal{E}}(n)   \\
%	           & = \ten{X}(n)\mmpr{U}\mmpr{A^\intercal}+\ten{\mathbfcal{E}}(n)
%\end{align*}
%From here, we can write $\ten{\mathbfcal{E}}(n)$ as
%\begin{align*}
%	\ten{\mathbfcal{E}}(n) & = \ten{X}(n) -	\ten{X}(n)\mmpr{\mat{U}}\mmpr{\mat{A}^\intercal} \\	                       & =	\ten{X}(n)(\ten{I}-\ten{I}\mmpr{\mat{U}}\mmpr{\mat{A}^\intercal})
%\end{align*}
%If we mutiply both sides with matrices $\mat{U}_k$, we obtain
%\begin{align*}
%	 & \ten{\mathbfcal{E}}(n)\mmpr{\mat{U}}                                                                   \\
%	 & \quad =  \ten{X}(n)(\ten{I}-\ten{I}\mmpr{\mat{U}}\mmpr{\mat{A}^\intercal})\mmpr{\mat{U}}              \\
%	 & \quad =	\ten{X}(n)(\ten{I}\mmpr{\mat{U}}-\ten{I}\mmpr{\mat{U}}\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}})
%\end{align*}
%and by Equation~\ref{eq:identity}
%\begin{equation}
%	\ten{\mathbfcal{E}}(n)\mmpr{\mat{U}} =
%	\ten{X}(n)(\ten{I}\mmpr{\mat{U}}-\ten{I}\mmpr{\mat{U}})
%	= 0
%\end{equation}
%
%
%From Equations~\ref{eq:proj-fwd-n} and~\ref{eq:uncorr}
%\begin{align*}
%	\ten{\Sigma}_{\ten{X}} & =	\ten{\Sigma}_{\ten{G}}\times_{1,2,\ldots,K}\{\mat{A}^\intercal\}
%	\times_{K+1,K+2,\ldots,2K}\{\mat{A}^\intercal\}                                             \\
%	                       & \quad+ \ten{\Sigma}_{\ten{\mathbfcal{E}}}
%\end{align*}
%leading to
%\begin{align*}
%	 & \ten{\Sigma}_{\ten{X}}\mmpr{\mat{U}}\ten{\Sigma}_{\ten{G}}^{-1}                                                 \\
%	 & \quad =(\{\mat{A}^\intercal\}\times\ten{\Sigma}_{\ten{G}}\mmpr{\mat{A}^\intercal} +
%	\ten{\Sigma}_{\ten{\mathbfcal{E}}})\mmpr{\mat{U}}\ten{\Sigma}_{\ten{G}}^{-1}                                       \\
%	 & \quad=
%	\{\mat{A}^\intercal\}\times\ten{\Sigma}_{\ten{G}}\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}}\ten{\Sigma}_{\ten{G}}^{-1} \\
%	 & \quad\quad + \ten{\Sigma}_{\ten{\mathbfcal{E}}}\mmpr{\mat{U}}\ten{\Sigma}_{\ten{G}}^{-1}                      \\
%	 & \quad= \{\mat{A}^\intercal\}\times\ten{\Sigma}_{\ten{G}}\ten{\Sigma}_{\ten{G}}^{-1}
%	+ \ten{\Sigma}_{\ten{\mathbfcal{E}}}\mmpr{\mat{U}}\ten{\Sigma}_{\ten{G}}^{-1}                                      \\
%	 & \quad= \ten{I}\mmpr{\mat{A}^\intercal} + 0\ten{\Sigma}_{\ten{G}}^{-1}                                          \\
%	 & \quad= \ten{I}\mmpr{\mat{A}^\intercal}
%\end{align*}

\section{Event-related potentials benchmark datasets}
\begin{table}[h]
  \scriptsize
	\begin{tabular}{@{}lrrcrl@{}}
		\toprule
		Dataset        & \# Sub.               & \# Chan. & \# Trials/class              & \# Sessions & Citation                   \\ \midrule
		BI2012         & 25                         & 16          & 640 NT / 128 T           & 2                          &                                          \\
		BI2013a        & 24                         & 16          & 3200 NT / 640 T
                   & 8 (Sub. 1-7) or 1  &                                          \\
		BI2014a        & 64                         & 16          & 990 NT / 198 T           & up to 3                    &                                          \\
		BI2014b        & 38                         & 32          & 200 NT / 40 T            & 3                          &                                          \\
		BI2015a        & 43                         & 32          & 4131 NT / 825 T          & 3                          &                                          \\
		BI2015b        & 44                         & 32          & 2160 NT / 480 T          & 1                          &                                          \\
		BNCI2014\_008  & 8                          & 8           & 3500 NT / 700 T          & 1                          &                                         \\
		BNCI2014\_009  & 10                         & 16          & 1440 NT / 288 T          & 3                          &                                         \\
		BNCI2015\_003  & 10                         & 8           & 1500 NT / 300 T          & 1                          &                                          \\
		Cattan2019\_VR & 21                         & 16          & 600 NT / 120 T           & 2                          &                                          \\
		EPFLP300       & 8                          & 32          & 2753 NT / 551 T          & 4                          &                                          \\
		Huebner2017    & 13                         & 31          & 364 NT / 112 T           & 3                          &                                          \\
		Huebner2018    & 12                         & 31          & 364 NT / 112 T           & 3                          &                                          \\
		Lee2019\_ERP   & 54                         & 62          & 6900 NT / 1380 T         & 2                          &                                          \\
		Sosulski2019   & 13                         & 31          & 75 NT / 15 T             & 3                          &                                          \\ \bottomrule
	\end{tabular}
\end{table}
\end{document}
