\documentclass[twocolumn]{article}

\usepackage[backend=biber]{biblatex}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{expl3}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{todonotes}
\usepackage{tabularx}
\usepackage[inline]{enumitem}

% Setup matplotlib pfg plots
\usepackage{pgf}
\def\mathdefault#1{#1}
\everymath=\expandafter{\the\everymath\displaystyle}
\makeatletter\@ifpackageloaded{underscore}{}{\usepackage[strings]{underscore}}\makeatother

% Layout
\setuptodonotes{inline}
\renewcommand*{\bibfont}{\footnotesize}

\addbibresource{references.bib}
\input{include/math.tex}
\input{include/tensorviz.tex}

% Metadata
\title{Block-Term Tensor Discriminant Analysis for Brain-Computer Interfacing}
\author{Arne Van Den Kerchove}

\begin{document}

\maketitle

\section{Introduction}
Brain-computer interfaces (BCIs) can replace, supplement, or enhance neural
communication pathways by enabling direct interaction between the brain and
external devices, with applications in the development of neuroprosthetics and
assistive technologies, among other fields~\cite{NicolasAlonso2012}.
To achieve their functionality, BCIs record and process neural data obtained
through neuroimaging techniques, with the electroencephalogram (EEG) being the
most commonly used method.

EEG data, like most neural signal acquisition modalities used for BCIs,
naturally exist as multi-channel time series, capturing information in both
spatial and temporal domains.
Common preprocessing transformations, such as time-frequency transformation,
time-binning, or integrating information across multiple subjects or conditions,
can further expand the data into additional analysis domains.
This can result in high-dimensional datasets, yet with a certain decoupling
between the domains.
Therefore, the intrinsic multiway structure of neural data~\cite{Erol2022} is
well-suited for representation as multiway arrays, or \emph{tensors}, which
provide a structured data representation that counteracts some of the drawbacks
resulting from this high dimensionality.

Machine learning techniques dealing with this datatype are referred to a tensor
methods.
Tensor methods can consider each analysis domain (tensor \emph{mode}) separately to
reduce a given problem into partial, per mode problems.
This has given rise to efficient dimensionality reduction techniques, such as
the Higher-Order Singular Value Decomposition
(\textsc{hosvd})~\cite{DeLathauwer2000} or Canonical Polyadic Decomposition
(\textsc{cpd})\todo{cite}.\todo{find examples of use in EEG/neural data/BCI and cite}

The former are all examples of unsupervised techniques with applications in EEG
processing.Given the specific task-related output required in the BCI applications, however,
supervised feature extraction and machine learning techniques are often of
interest in this field.

One approach is to incorporate some assumptions of the tensor structure of the
data directly into the estimation of parameters of classic linear machine
learning methods, such as in Linear Discriminant Analysis or beamforming.
Advances have been made by leveraging the decoupling of the spatial and temporal
domains in EEG event-related potential (ERP) classification using Spatiotemporal
Discriminant analysis~\cite{Li2010,Zhang2013} or methods regularizing covariance
matrix estimation~\cite{Kerchove2022,Sosulski2022}.\todo{include this
paragraph?}

A more structured approach to the same problem is to design a supervised
tensor dimensionality tensor method that optimizes discriminability between the
extracted features, as is the case for Higher Order Discriminant
Analysis (\textsc{hoda})~\cite{Yan2005,Phan2010,Froelich2018}.
These extracted features can subsequently be further classified, most commonly
using LDA or a support vector machine (SVM) to obtain predictions.
Variants of \textsc{hoda} have been applied to BCI problems such as
ERP~\cite{Onishi2012,Higashi2016} and motor imagery (MI)~\cite{Liu2015,Cai2021}.
Recent adaptations improve on these results by using suited objective
functions and regularization, such as in Higher order spectral regression
discriminant analysis~\cite{Jamshidi2017}, Spatiotemporal Linear
Feature~\cite{Aghili2023}, Oscillatory source Tensor Discriminant
Analysis~\cite{Jorajuria2022}.

The previous methods adhere to the \textsc{tucker} tensor decomposition
structure, meaning that they reduce input tensors of size
$(D_1,D_2,\ldots,D_K)$ to a smaller tensor of size $(r_1,r_2,\ldots,r_K)$ with
each $r_k\leq D_k$, similar to \textsc{hosvd}.
While effective, other structures such as the \textsc{parafac} employed in
\textsc{cpd}, where a tensor is decomposed into a sum of rank-1 tensors,
might be more appropriate to represent the neural data of interest.
Discriminant tensor features can also be extracted
in the \textsc{parafac} structure with, for instance using manifold
optimization~\cite{Froelich2018}.

Nevertheless, the \textsc{parafac} structure might still not be able to
efficiently represent all relevant information in a compressed format.
The block-term tensor structure is a generalization of the \textsc{tucker} and
\textsc{parafac} structures, and can be calculated in an unsupervised way using
the Block-term Tensor Decomposition
(\textsc{btd})~\cite{DeLathauwer2008,DeLathauwer2008a,DeLathauwer2008b,Rontogiannis2021}.
\textsc{btd}, of which the \textsc{hosvd} and \textsc{cpd} are special cases,
represents a tensor a sum of \textsc{tucker} terms.
Research has shown that this more flexible structure can improve BCI performance
when adapted to supervised methods, such as in Higher-Order Parial Least
Squares~\cite{Camarrone2018} or Block-Term Tensor Regression (\textsc{bttr})~\cite{Faes2022,Faes2022b}
\textsc{bttr} has been adapted into a classification variant, named Block-Term
Tensor Classification (\textsc{bttc})~\cite{Camarrone2021}, but since features
are not directly optimized for class separability but rather regressed towards
a dummy independent variable, results can be improved upon and the method
cannot be extended to a multi-class setting.
Furthermore, structures employed in \textsc{hopls}, \textsc{bttdr} and
\textsc{bttc} are still more constrained than what could be achieved with a
full block-term tensor structured decomposition optimized for discriminability.

\todo{talk about spectrum-weighted tda~\cite{Huang2020}, criticize structure}

In this work, we propose the following contributions:
\begin{enumerate*}[label={\arabic*)}]
  \item first, we develop a forward model for \textsc{hoda} to reconstruct a
    given input tensor from the extracted features.
  \item This allows us to introduce a state-of-the-art BCI classification method based on the
    block-term tensor structure, named Block-Term Tensor Discriminant Analysis
    (\textsc{BTTDA}) and
  \item evaluate this decoder together with other tensor methods and the
    special \textsc{parafac}-structured case on an extensive benchmark of BCI
    datasets for ERP and MI decoding).
\end{enumerate*}

\section{Methods}

\subsection{Notation}
Tensors are indicated as bold underlined letters $\ten{X}$, matrices as bold
letters $\mat{U}$, fixed scalars as uppercase letters $K$ and variable
scalars as lower case letters $k$.
The $n^\text{th}$ sample of a tensor dataset with $N$ samples is written as
$\ten{X}(n)$.
A tensor $\ten{X}\in \mathbb{R}^{D_1\times D_2 \times \ldots \times D_K}$ can be unfolded in mode
$k$ to a matrix $\mat{X}_k\in\mathbb{R}^{(D_k\times\prod_{j\neq k}^K D_j)}$.
matrix
The tensor-matrix product of tensor $\ten{X}$ with matrix $\mat{U}$ along a
given mode $k$ is written as $\ten{X}\mpr{\mat{U}}{k}$. For ease of notation, let
$\ten{X}\mmpr{\mat{U}} =
	\ten{X}\mpr{\mat{U}}{1}\mpr{\mat{U}}{2}\cdots\mpr{\mat{U}}{K}$.
When skipping one of the modes $k$, this is
written as $\ten{X}\mmprs{\mat{U}}{k} =
	\ten{X}\mpr{\mat{U}}{1}\mpr{\mat{U}}{2}\cdots\mpr{\mat{U}}{k-1}\mpr{\mat{U}}{k+1}\ldots\mpr{\mat{U}}{K}$.
%The Kronecker product is noted as $\otimes$, covariance matrices are indicated  with $\mat{\Sigma}$.

\subsection{Higher-Order Discriminant Analysis (\textsc{hoda})}
Higher Order Discriminant Analysis (\textsc{hoda})~\cite{Phan2010} is a tensor
feature extraction technique. For a $K^{th}$ order input tensor $\ten{X}$ of
shape $(D_1,D_2,\ldots,D_K)$, \textsc{hoda} finds projection matrices $\mat{U_k}$ for each mode $k$
that project $\ten{X}$ a core tensor $\ten{G}$, usually with lower
dimensionality $(r_1,r_2,\ldots,r_K)$ using tensor-matrix mode products
\begin{equation}
	\ten{G}  = \ten{X}\mmpr{\mat{U}}
	\label{eq:hoda-backward}
\end{equation}
visualized in Figure~\ref{fig:hoda-backward}.
\begin{figure}
	\centering
	\input{figures/hoda_backward.tikz.tex}
	\caption{A visualization of the multilinear projection learnt by Higher Order
		Discriminant Analysis (\textsc{hoda}) for a dataset of $N$ second order tensors
		$\ten{X}$ of shape $(D_1,D_2)$.
		\textsc{hoda} finds projection matrices $\mat{U}_k$ such that maximal
		discriminability between classes is achieved in the projected core tensors
		$\ten{G}$ with reduced dimensionality $(r_1,r_2)$.}
	\label{fig:hoda-backward}
\end{figure}
Analogous to the \textsc{tucker} decomposition, \textsc{hoda} is a dimensionality
reduction decomposition that results in a dense core tensor $\ten{G}$, and
imposes an orthogonality constraint on $\mat{U}_k$ to ensure uniqueness.
However, while for the \textsc{tucker} decomposition the projection matrices
are chosen to minimize the reconstruction error, the projection matrices
$\mat{U}_k$ of \textsc{hoda} are optimized for maximal discriminability between
$\ten{G}(n)$ belonging corresponding classes with labels $c_n$.
This is a desirable property in a classification setting where samples are
high-dimensional tensors.

\textsc{hoda} optimizes discriminability in the Fisher sense, by optimizing the
Fisher ratio $\phi$ between the core tensors $\ten{G}(n)$
\begin{equation}
  \left\{\mat{U}^*\right\} = \argmax_{\{\mat{U}\}}
	\frac{\sum_c^CN_c\left\lVert\bar{\ten{G}}(c)-\bar{\bar{\ten{G}}}\right\rVert_F^2}
	{\sum_n^N\left\lVert\ten{G}(n)-\bar{\ten{G}}(c_n)\right\rVert_F^2}
	\label{eq:fisher}
\end{equation}
for $C$ classes with each $N_c$ samples. $\bar{\ten{G}}(c)$ is the mean of core
tensors $\ten{G}(n)$ of class $c$, and $\bar{\bar{\ten{G}}}$ the mean of
these class mean core tensors.
Equation~\ref{eq:fisher} can be solved by the iterative algorithm in
Algorithm~\ref{alg:hoda}.
To start, $\mat{U}_k$ are initialized to orthogonal matrices, e.g. as random
orthogonal matrices, by a per mode Singular Value Decomposition (\textsc{svd})
or by a Multi-Linear Singular Value Decomposition (\textsc{mlsvd})~\cite{Lathauwer2000} of the input data.
At each iteration, the algorithm loops trough the modes and fixes all the
projection of all but $\mat{U}_k$ corresponding to mode $k$.
It then finds a partial core tensor $\ten{G}_{-k}=\ten{X}\mmprs{\mat{U}}{k}$
Subsequently, a new projection matrix $U_k$ can be found analogous to Linear
Discriminant Analysis by constructing the within- and between-class scatter
matrices $\mat{S}_{-k,\text{w}}$ and $\mat{S}_{-k,\text{b}}$ \todo{equations
for scatter matrices} of
$\mat{G}_{-k,k}$ the partial core tensor unfolded towards mode $k$, and solving
the generalized eigenvalue problem
\begin{equation}
  \mat{S}_{-k,\text{b}}\mat{U}_k = \mat{\lambda}\mat{S}_{-k,\text{w}}\mat{U}_k
\end{equation}
The iterative process halts after a fixed amount or iterations, or when the
update of each $\mat{U}_k$ is lower than a predetermined threshold.

To apply \textsc{hoda} in a classification setting, the projections $\mat{U}_k$
can first be learned on a training dataset
$\left\{\ten{X}^\text{train}(m)\right\}_m^M$ with known class labels, and
training features $\left\{\ten{G}^\text{train}(m)\right\}_m^M$ can be extracted.
Next, these training features are reshaped (vectorized) into vectors
$\left\{\vec{g}^\text{train}(m) \in \mathbb{R}^{\prod_k^Kr_k}\right\}_m^M$ and a classifier is
trained on these features and the corresponding class labels.
Finally, the learnt projections $\mat{U}_k$ can be applied to unseen testing
dataset $\left\{\ten{X}(n)^\text{test}\right\}_n^N$ and the exctracted features
$\left\{\ten{G}^\text{test}(n)\right\}_n^N$ vectorized into
$\left\{\vec{g}^\text{train}(n)\right\}_n^N$ can be
classified by the trained classifier.\todo{remove set notations for brevity}
To avoid overfitting and improve performance in low sample size settings, the
\textsc{HODA} problem can be regularized by shrinking the the partial
whithin-class scatter matrix~\cite{Phan2010} with a shrinkage factor
$\alpha_k$ at each step such that the generalized eigenvalue problem becomes
\begin{equation}
	\mat{S}_b^{(-k)}\mat{U}_k =
\mat{\lambda}\left[(1-\alpha_k)\mat{S}_w^{(-k)}-\alpha_k\mat{I}\right]\mat{U}_k
\end{equation}
Like in Linear Discriminant Analysis, the shrinkage parameter $\alpha_k$ can
also be estimated in a data driven way in \textsc{hoda}~\cite{Jorajuria2022},
e.g., using the Ledoit-Wolf procedure~\cite{Ledoit2003}.


\subsection{A forward model for \textsc{hoda}}
As a prerequisite for our proposed \textsc{bttda} model, we must first find a
way to reconstruct the original data tensor $\ten{X}$ as accurately as possible
from $\ten{G}$ after dimensionality reduction.
In neuroimaging, this is referred to as a \textit{forward model}.
While a \textit{backward model} extracts latent sources or properties from the observed
data, which can be optimised for tasks like regression or classification or
informed by prior knowledge about source propagation, a \textit{forward model} is a
generative model that expresses the observed data in function of some latent
properties or sources that are given.
\todo{interpretability and generative model}

The \textsc{hoda} projection in Equation~\ref{eq:hoda-backward} is an example
of a backward model.
A straightforward and computationally efficient candidate for a corresponding
forward model is
\begin{equation}
	\ten{X} = \ten{G}\mmpr{\mat{A}^\intercal} + \ten{\mathbfcal{E}}
	\label{eq:hoda-forward}
\end{equation}
with \textit{activation patterns} $\mat{A}_k \in \mathbb{R}^{D_k\times r_k}$
and error term $\ten{\mathbfcal{E}}$.
\begin{figure}
	\centering
	\input{figures/hoda_forward.tikz.tex}
	\caption{The forward projection for \textsc{hoda}. Leveraging activation
		patterns $A_k$, the original tensor $\ten{X}$ can approximately be
		reconstructed from projected core tensor of latent factors $\ten{G}$. $A_k$ are chosen such
		that the variability captured in the latent factors is maximally expressed in
		the reconstructed tensor and not in the error term.}
	\label{fig:hoda-forward}
\end{figure}

A forward model should make sure that reconstruction error is minimized and
variation captured in the latent factors is maximally captured by the forward
projection term $\ten{G}\mmpr{\mat{A}^\intercal}$, and not by the error term
$\ten{\mathbfcal{E}}$~\cite{Haufe2014}.
Hence, we aim to minimize the expected value of the cross-covariance between
the noise term and the latent factors\todo{consistently use 'core tensor' or
'latent factors'}
\begin{equation}
  \left\{\mat{A}^*\right\} = \arg\min_{\{\mat{A}\}}\text{E}\left[\text{vec}\left({\ten{\mathbfcal{E}}(n)}\right)\text{vec}\left({\ten{G}(n)}\right)\right]_n
\end{equation}
or, equivalently~\cite{Parra2005,Haufe2014},
\begin{equation}
  \left\{\mat{A}^*\right\} = \arg\min_{\{\mat{A}\}}\sum_n^N\left(\ten{X}(n) - \ten{G}(n)\mmpr{\mat{A}}\right)^2
\end{equation}
This least squares tensor approximation problem can be solved efficiently using the
alternating least squares (ALS) algorithm~\cite{Comon2009},
iteratively fixing all but one of the activation patterns such that
\begin{equation}
  \mat{A}_k = \arg\min_{\mat{A}_k}
  \sum_n^N\left[\mat{X}_k(n) -
  \left(\ten{G}(n)\mmprs{\mat{A}}{k}\right)_k\right]^2
\end{equation}
at every iteration, which can be solved directly by ordinary least squares.
The activation patterns are initialized to the weights $\{\mat{U}\}$ of the
backward model.
Similar to fitting the backward model, the iterative process for the forward
model halts after a fixed amount of iterations or when the update of each
$\mat{A}_k$ is lower than a predetermined threshold.
\todo{pseudocode}

\subsection{Block-Term Tensor Discriminant Analysis (\textsc{bttda})}
After defining the forward model, we can construct our proposed block-term
tensor model. Assuming the core tensors $\ten{G}$
obtained by the backward projection of \textsc{hoda} do not achieve perfect
class separation, the error term $\ten{\mathbfcal{E}}$ in
Equation~\ref{eq:hoda-forward} should still contain some discriminative
information, which can be exploited to improve classifier performance.
We thus extend the \textsc{hoda} feature extraction scheme with backward an
forward models defined in respectively Equations~\ref{eq:hoda-backward}
and~\ref{eq:hoda-forward} to Block-Term Tensor Discriminant Analysis
(\textsc{bttda}), with a forward model given by
\begin{align}
  \ten{X} & = \sum_b^B\ten{G}^{(b)}\mmpr{\mat{U}^{(b)}} + \ten{\mathbfcal{E}}
  \label{eq:bttda-forward}
\end{align}
which extracts $B$ core tensors $\ten{G}^{(b)}$ from input tensor $\ten{X}$
until error term $\ten{\mathbfcal{E}}$ remains.
Figure~\ref{fig:bttda} further illustrates the \textsc{bttda} model.
\begin{figure*}
	\centering
	\input{figures/bttda.tikz.tex}
  \caption{A forward model for Block-Term Tensor Discriminant Analysis
  (\textsc{bttda}). \textsc{bttda} can extract more features
 than \textsc{hoda} by iteratively finding a core tensor $\ten{G}^{(b)}$ in a
 deflation scheme.
 The \textsc{hoda} backward projection is first applied. Next, the
 input data is reconstructed via the \textsc{hoda} forward model and the
 difference between the two is found.
 Finally, this process is repeated with this difference as input data, until a
 desired number of blocks $B$ has been found.}
  \label{fig:bttda}
\end{figure*}

Since \textsc{bttda} is specified as a forward model, a backward modelling
procedure is required which finds the core tensors $\ten{G}^{(b)}$ given $\ten{X}$ for
\textsc{bttda} to be useful as a feature extraction method.
The extracted features represented by the core tensors $\ten{G}^{(b)}$ can be
computed through a deflation scheme summarized in Algorithm~\ref{alg:bttda}.
For each block $b$, the core tensor is extracted using the \textsc{hoda} backward
projection in from a residual error term
$\ten{\mathbfcal{E}}^{(b)}$
\begin{equation}
  \ten{G}^{(b)} = \ten{\mathbfcal{E}}^{(b)}\mmpr{\mat{U}^{(b)}}
\end{equation}
This residual error term is calculated by finding the difference between the
previous error and its reconstruction after backward and forward \textsc{hoda}
projection
\begin{equation}
  \ten{\mathbfcal{E}}^{(b+1)} = \ten{\mathbfcal{E}}^{(b)} - \ten{G}^{(b)}
  \mmpr{\mat{A}^{\intercal(b)}}
\end{equation}
with $\ten{\mathbfcal{E}}^{(1)}=\ten{X}$.

The resulting extracted feature tensors can be flattened and concatenated into
one feature vector, so that they can be classified in a similar manner to
\textsc{hoda}.


\subsection{Model and feature selection}
Similar to the unsupervised \textsc{btd}, the performance of \textsc{bttda} is
heavily dependent on the rank $\left(r_1^{(b)}, r_2^{(b)}, \ldots,
r_K^{(b)}\right)$ and on the number of blocks $B$.
If these are not kown a priori, i.e. if they cannot be set based on insights in the
data generation process, a model selection step is necessary in order to
determine what the optimal values for $r_k^{(b)}$ and $B$ are.
Hyperparameter tuning through cross-validation is straight-forward solution to
determine these parameters, yet can be computationally expensive.
To reduce the computational cost, Algorithm~\ref{alg:model-selection} proposes a model selection algorithm that
leverages cross-validation in a greedy way per block, to iteratively find the
optimal rank for the next block given the ranks of the previous block the previous blocks.
\todo{Algorithm}
\todo{How to determine $B$}

\textsc{hoda} can extract a substantial amount of redundant features, that can
be dropped after projection and before proceeding to the classification
step~\cite{Phan2010}.
Since \textsc{bttda} extracts a set of \textsc{hoda}
feature tensors, these redundant features can stack up over the number of
blocks.
Therefore, we apply feature selection after finding all \textsc{bttda} blocks
by calculating the univariate Fisher score of the concatenated vectorized
features, and retain only those features that have  statistical significant
contribution to class separation with $p<\alpha$.
Since the extracted features are already optimized for class separation,
$\alpha$ should not be chosen too low.
The significance thershold in this work is set to $\alpha=0.95$, for all
evaluations of both \textsc{hoda} and \textsc{bttda}.


\section{Experiments}
\subsection{Datasets \& decoders}
We evaluate our proposed model in two off-line BCI decoding setting.
MOABB~\cite{Aristimunha2023} is an openly available BCI benchmarking platform
containing multiple EEG datasets for different BCI tasks.
Specifically, we will test our decoder on Event-Related Potential (ERP) tasks
such as the P300 paradigm, and on Motor Imagery (MI) tasks.
\subsection{Block contribution}
Decrease of MSE with blocks
increase of f ratio with blocks
increase of validation score with blocks
\begin{figure*}
  \input{figures/blocks.pgf}
\end{figure*}
\subsection{Event-Related Potentials (ERPs)}
ERPs are spatiotemporal features, with each sample forming a $2^\text{nd}$
order tensor with 2 modes representing the channels and time samples.
\begin{table*}
  \footnotesize
  \input{include/score_erp.tex}
  \caption{Performances as described in \cite{Chevallier2024}}
\end{table*}
\subsection{Motor Imagery (MI)}
For MI, discriminatory information is represented in the EEG data as
Event-Related Synchronizations/Desynchronizations (ERS/Ds).
Contrary the the time domain analysis performed on ERPs, ERS/Ds are only
detectable in the time-frequency domain.
Hence, for the MI task, we will transform the EEG signal into the
time-frequency domain, forming $3^\text{d}$ order tensors, with modes
representing the channels, the frequencies and time samplies and time samples.

\begin{table}
  \caption{Performances as described in \cite{Chevallier2024}}
\end{table}
\section{Discussion}


%https://sci-hub.ru/https://ieeexplore.ieee.org/abstract/document/6287946
%https://www.sciencedirect.com/science/article/abs/pii/S0031320317301875
\printbibliography

\appendix
%\section{Proof of Theorem~\ref{the:ap}}
%
%Equations~\ref{eq:hoda-backward} and~\ref{eq:hoda-forward} can be expressed in fuction
%of sample $n$ as
%\begin{subequations}
%	\label{eq:proj-n}
%	\begin{align}
%		\ten{G}(n) & = \ten{X}(n)\mmpr{\mat{U}}
%		\label{eq:proj-back-n}                                                   \\
%		\ten{X}(n) & = \ten{G}(n)\mmpr{\mat{A}^\intercal}+\ten{\mathbfcal{E}}(n)
%		\label{eq:proj-fwd-n}
%	\end{align}
%\end{subequations}
%%We write Equations~\ref{eq:proj-back-n} and~\ref{eq:proj-fwd-n} respectively
%%as their unfolded multi-mode products for mode $k$
%%\begin{subequations}
%%	\label{eq:proj-unfold}
%%	\begin{align}
%%		%https://www5.in.tum.de/persons/huckle/tensor-kurs_1.pdf
%%		\mat{G}_k(n) & =
%%		\mat{U}_n\mat{X}_n(n)\left(\mat{U}_1\otimes\mat{U}_2\otimes\cdots\otimes\mat{U}_{k-1}\otimes\mat{U}_{k+1}\otimes\cdots\otimes\mat{U}_K\right)
%%		\label{eq:proj-back-unfold} \\
%%		\mat{X}_k(n) & =
%%		\mat{A}_n^\intercal\mat{G}_n(n)\left(\mat{A}^\intercal_1\otimes\mat{A}^\intercal_2\otimes\cdots\otimes\mat{A}^\intercal_{k-1}\otimes\mat{A}^\intercal_{k+1}\otimes\cdots\otimes\mat{A}^\intercal_K\right)
%%		+ \mat{\mathbfcal{E}}_k(n)
%%		\label{eq:proj-fwd-unfold}
%%	\end{align}
%%\end{subequations}
%%According to Theorem 1 in~\cite{Haufe2014}, the activation patterns defining the
%%unfolded forward model in Equation~\ref{eq:proj-fwd-unfold} corresponding to the
%%unfolded backward model in Equation~\ref{eq:proj-back-unfold} is given by
%%\begin{align}
%%	\mat{A}_k = \mat{\Sigma}_{\mat{X}_k}\mat{U}_k
%%\end{align}
%Let us now express these backward and forward models in their vectorized forms:
%\begin{subequations}
%	\label{eq:proj-n}
%	\begin{align}
%		\vec{g}(n) & = \vec{x}(n)\left(\bigotimes_k^K\mat{U}_k\right)
%		\label{eq:proj-back-vec}                                                  \\
%		\vec{x}(n) & = \vec{g}(n)\left(\bigotimes_k^K\mat{A}_k^\intercal\right) +
%		\vec{\boldsymbol\epsilon}(n)
%		\label{eq:proj-fwd-vec}
%	\end{align}
%\end{subequations}
%Since $\bigotimes_k^K\mat{U}_k$ is a Kronecker product of orthogonal matrices,
%which itself is orthogonal, the activation pattern
%$\mat{B}\in\mathbb{R}^{\prod_k^KD_k\times\prod_k^Kr_k} $ of a vectorized forward model
%corresponding to Equation~\ref{eq:proj-back-vec} is given according
%to~\cite{Haufe2014} as
%\begin{align*}
%	\mat{B} & =
%	\mat{\Sigma}_\vec{x}\left(\bigotimes_k^K\mat{U}_k\right)\mat{\Sigma}_\vec{g}^{-1} \\
%	        & =
%	\mat{\Sigma}_\vec{x}\left(\bigotimes_k^K\mat{U}_k\right)\left[\left(\bigotimes_k^K\mat{U}_k\right)^\intercal\mat{\Sigma_\vec{x}}\left(\bigotimes_k^K\mat{U}_k\right)\right]^{-1}
%\end{align*}
%
%Because \textsc{hoda} assumes the data covariance can be expressed as a
%Kronecker product of mode-$k$ covariances
%$\mat{\Sigma}_{\mat{X}_k}$\todo{citation  needed}, we get
%\begin{align*}
%	\mat{B} & =
%	\left(\bigotimes_k^K\mat{\Sigma}_{\mat{X}_k}\right)\left(\bigotimes_k^K\mat{U}_k\right)
%  \\
%          & \quad \cdot
%          \left[\left(\bigotimes_k^K\mat{U}_k\right)^\intercal\left(\bigotimes_k^K\mat{\Sigma}_{\mat{X}_k}\right)\left(\bigotimes_k^K\mat{U}_k\right)\right]^{-1}
%	\\
%	        &
%	=\left(\bigotimes_k^K\mat{\Sigma}_{\mat{X}_k}\right)\left(\bigotimes_k^K\mat{U}_k\right)\left(\bigotimes_k^K\mat{U}_k^\intercal\mat{\Sigma}_{\mat{X}_k}\mat{U}_k\right)^{-1} \\
%	        &
%	=\left(\bigotimes_k^K\mat{\Sigma}_{\mat{X}_k}\right)\left(\bigotimes_k^K\mat{U}_k\right)\left[\bigotimes_k^K\left(\mat{U}_k^\intercal\mat{\Sigma}_{\mat{X}_k}\mat{U}_k\right)^{-1}\right]
%	\\
%	        & = \bigotimes_k^K \mat{\Sigma}_{\mat{X}_k}
%	\mat{U}_k\left(\mat{U}_k^\intercal\mat{\Sigma}_{\mat{X}_k}\mat{U}_k\right)^{-1}
%\end{align*}
%and finally
%\begin{align*}
%  \mat{A}_k = \mat{\Sigma}_{\mat{X}_k}
%	\mat{U}_k\left(\mat{U}_k^\intercal\mat{\Sigma}_{\mat{X}_k}\mat{U}_k\right)^{-1}
%\qquad\blacksquare
%\end{align*}



%Our proof follows the structure laid out in \cite{Haufe2014} (Appendix A).
%In the general case when $r_k<D_k$, $\mat{U}_k$ are not square and hence non-invertible.
%The backward and forward projections given by
%Equations~\ref{eq:hoda-backward} and~\ref{eq:hoda-forward} can be expressed in fuction
%of sample $n$ as
%\begin{equation}
%	\ten{G}(n) = \ten{X}(n)\mmpr{\mat{U}}
%	\label{eq:proj-back-n}
%\end{equation}
%and
%\begin{equation}
%	\ten{X}(n) = \ten{G}(n)\mmpr{\mat{A}^\intercal}+\ten{\mathbfcal{E}}(n)
%	\label{eq:proj-fwd-n}
%\end{equation}
%respectively, with unkown error term $\ten{\mathbfcal{E}}(n)$.
%
%Plugging the forward projection in Equation~\ref{eq:proj-fwd-n} into the backward projection in
%Equation~\ref{eq:proj-back-n} gives
%\begin{align*}
%	\ten{G}(n) & = \ten{X}(n)\mmpr{\mat{U}}                           \\
%	           & = (\ten{G}(n)\mmpr{\mat{A}^\intercal} +
%	\ten{\mathbfcal{E}}(n))\mmpr{\mat{U}}                             \\
%	           & = \ten{G}(n)\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}} +
%	\ten{\mathbfcal{E}}(n)\mmpr{\mat{U}}
%\end{align*}
%Taking the tensor outer product on the left with $\ten{G}$ yields
%\begin{align*}
%	\ten{G}(n)\otimes\ten{G}(n) & = \ten{G}(n)\otimes\ten{G}(n)\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}} \\
%	                            & \quad +	\ten{G}(n)\otimes\ten{\mathbfcal{E}}(n)\mmpr{\mat{U}}
%\end{align*}
%and when taking the expected value over samples
%\begin{align*}
%	 & \ev{\ten{G}(n)\otimes\ten{G}(n)}{n}                                                \\
%	 & \quad = \ev{\ten{G}(n)\otimes\ten{G}(n)\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}}}{n} \\
%	 & \quad\quad + \ev{\ten{G}(n)\otimes\ten{\mathbfcal{E}}(n)\mmpr{\mat{U}}}{n}       \\
%	 & \quad = \ev{\ten{G}(n)\otimes\ten{G}(n)}{n}\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}} \\
%	 & \quad\quad +  \ev{\ten{G}(n)\otimes\ten{\mathbfcal{E}}(n)}{n}\mmpr{\mat{U}}
%\end{align*}
%To find a forward projection that is corresponding to the backward projection,
%ensuring any variation explained by the projections is maximally captured in the
%activation patterns, we assume
%\begin{equation}
%	\ev{\ten{G}(n)\otimes\ten{\mathbfcal{E}}(n)}{n} = 0
%	\label{eq:uncorr}
%\end{equation}
%yielding
%\begin{align*}
%	 & \ev{\ten{G}(n)\otimes\ten{G}(n)}{n}                                                \\
%	 & \quad = \ev{\ten{G}(n)\otimes\ten{G}(n)}{n}\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}}
%\end{align*}
%Since the covariance tensor of the latent tensor,
%$\ev{\ten{G}(n)\otimes\ten{G}(n)}{n}$,
%has full tensor rank due to the linear independance of columns in projection
%matrices $\mat{U}_k$, and it exists in $\mathbb{R}^{r_1\times r_1\times
%		r_2\times \ldots\times r_K\times r_1\times r_2\times \ldots\times r_K}$, its tensor inverse exists and we derive
%\begin{equation}
%	\ten{I} = \ten{I}\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}}
%	\label{eq:identity}
%\end{equation}
%
%Plugging in the backward projection in Equation~\ref{eq:proj-back-n} into the
%forward projection in Equation~\ref{eq:proj-fwd-n} gives
%\begin{align*}
%	\ten{X}(n) & = \ten{G}(n)\mmpr{\mat{A^\intercal}}+\ten{\mathbfcal{E}}(n)   \\
%	           & = \ten{X}(n)\mmpr{U}\mmpr{A^\intercal}+\ten{\mathbfcal{E}}(n)
%\end{align*}
%From here, we can write $\ten{\mathbfcal{E}}(n)$ as
%\begin{align*}
%	\ten{\mathbfcal{E}}(n) & = \ten{X}(n) -	\ten{X}(n)\mmpr{\mat{U}}\mmpr{\mat{A}^\intercal} \\	                       & =	\ten{X}(n)(\ten{I}-\ten{I}\mmpr{\mat{U}}\mmpr{\mat{A}^\intercal})
%\end{align*}
%If we mutiply both sides with matrices $\mat{U}_k$, we obtain
%\begin{align*}
%	 & \ten{\mathbfcal{E}}(n)\mmpr{\mat{U}}                                                                   \\
%	 & \quad =  \ten{X}(n)(\ten{I}-\ten{I}\mmpr{\mat{U}}\mmpr{\mat{A}^\intercal})\mmpr{\mat{U}}              \\
%	 & \quad =	\ten{X}(n)(\ten{I}\mmpr{\mat{U}}-\ten{I}\mmpr{\mat{U}}\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}})
%\end{align*}
%and by Equation~\ref{eq:identity}
%\begin{equation}
%	\ten{\mathbfcal{E}}(n)\mmpr{\mat{U}} =
%	\ten{X}(n)(\ten{I}\mmpr{\mat{U}}-\ten{I}\mmpr{\mat{U}})
%	= 0
%\end{equation}
%
%
%From Equations~\ref{eq:proj-fwd-n} and~\ref{eq:uncorr}
%\begin{align*}
%	\ten{\Sigma}_{\ten{X}} & =	\ten{\Sigma}_{\ten{G}}\times_{1,2,\ldots,K}\{\mat{A}^\intercal\}
%	\times_{K+1,K+2,\ldots,2K}\{\mat{A}^\intercal\}                                             \\
%	                       & \quad+ \ten{\Sigma}_{\ten{\mathbfcal{E}}}
%\end{align*}
%leading to
%\begin{align*}
%	 & \ten{\Sigma}_{\ten{X}}\mmpr{\mat{U}}\ten{\Sigma}_{\ten{G}}^{-1}                                                 \\
%	 & \quad =(\{\mat{A}^\intercal\}\times\ten{\Sigma}_{\ten{G}}\mmpr{\mat{A}^\intercal} +
%	\ten{\Sigma}_{\ten{\mathbfcal{E}}})\mmpr{\mat{U}}\ten{\Sigma}_{\ten{G}}^{-1}                                       \\
%	 & \quad=
%	\{\mat{A}^\intercal\}\times\ten{\Sigma}_{\ten{G}}\mmpr{\mat{A}^\intercal}\mmpr{\mat{U}}\ten{\Sigma}_{\ten{G}}^{-1} \\
%	 & \quad\quad + \ten{\Sigma}_{\ten{\mathbfcal{E}}}\mmpr{\mat{U}}\ten{\Sigma}_{\ten{G}}^{-1}                      \\
%	 & \quad= \{\mat{A}^\intercal\}\times\ten{\Sigma}_{\ten{G}}\ten{\Sigma}_{\ten{G}}^{-1}
%	+ \ten{\Sigma}_{\ten{\mathbfcal{E}}}\mmpr{\mat{U}}\ten{\Sigma}_{\ten{G}}^{-1}                                      \\
%	 & \quad= \ten{I}\mmpr{\mat{A}^\intercal} + 0\ten{\Sigma}_{\ten{G}}^{-1}                                          \\
%	 & \quad= \ten{I}\mmpr{\mat{A}^\intercal}
%\end{align*}

\section{Benchmark datasets}
\begin{table*}[h]
\begin{tabularx}{\linewidth}{@{}Xrrcrl@{}}
		\toprule
		Dataset        & \# Sub.               & \# Chan. & \# Trials/class              & \# Sessions & Citation                   \\ \midrule
		BI2012         & 25                         & 16          & 640 NT / 128 T           & 2                          &                                          \\
		BI2013a        & 24                         & 16          & 3200 NT / 640 T
                   & 8 (Sub. 1-7) or 1  &                                          \\
		BI2014a        & 64                         & 16          & 990 NT / 198 T           & up to 3                    &                                          \\
		BI2014b        & 38                         & 32          & 200 NT / 40 T            & 3                          &                                          \\
		BI2015a        & 43                         & 32          & 4131 NT / 825 T          & 3                          &                                          \\
		BI2015b        & 44                         & 32          & 2160 NT / 480 T          & 1                          &                                          \\
		BNCI2014\_008  & 8                          & 8           & 3500 NT / 700 T          & 1                          &                                         \\
		BNCI2014\_009  & 10                         & 16          & 1440 NT / 288 T          & 3                          &                                         \\
		BNCI2015\_003  & 10                         & 8           & 1500 NT / 300 T          & 1                          &                                          \\
		Cattan2019\_VR & 21                         & 16          & 600 NT / 120 T           & 2                          &                                          \\
		EPFLP300       & 8                          & 32          & 2753 NT / 551 T          & 4                          &                                          \\
		Huebner2017    & 13                         & 31          & 364 NT / 112 T           & 3                          &                                          \\
		Huebner2018    & 12                         & 31          & 364 NT / 112 T           & 3                          &                                          \\
		Lee2019\_ERP   & 54                         & 62          & 6900 NT / 1380 T         & 2                          &                                          \\
		Sosulski2019   & 13                         & 31          & 75 NT / 15 T             & 3                          &                                          \\ \bottomrule
	\end{tabularx}
\end{table*}
\end{document}
