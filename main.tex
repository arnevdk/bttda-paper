\documentclass[twocolumn]{article}

\usepackage[backend=biber]{biblatex}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{expl3}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{todonotes}
\usepackage{tabularx}
\usepackage[inline]{enumitem}
\usepackage{float}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage{algpseudocode}

% Setup matplotlib pgf plots
\usepackage{pgf}
\def\mathdefault#1{#1}
\everymath=\expandafter{\the\everymath\displaystyle}
\makeatletter\@ifpackageloaded{underscore}{}{\usepackage[strings]{underscore}}\makeatother

% Layout
\setuptodonotes{inline}
\renewcommand*{\bibfont}{\footnotesize}
\newfloat{algorithm}{t}{lop}

\addbibresource{references.bib}
\input{include/math.tex}
\input{include/tensorviz.tex}

% Metadata
\title{Block-Term Tensor Discriminant Analysis for Brain-Computer Interfacing}
\author{Arne Van Den Kerchove}

\begin{document}

\maketitle

\section{Introduction}
Brain-computer interfaces (BCIs) can replace, supplement, or enhance neural
communication pathways by enabling direct interaction between the brain and
external devices, with applications in the development of neuroprosthetics and
assistive technologies, among other fields~\cite{NicolasAlonso2012}.
To achieve their functionality, BCIs record and process neural data obtained
through neuroimaging techniques, with the electroencephalogram (EEG) being the
most commonly used method.

EEG data, like most neural signal acquisition modalities used for BCIs,
naturally exist as multi-channel time series, capturing information in both
spatial and temporal domains.
Common preprocessing transformations, such as time-frequency transformation,
time-binning, or integrating information across multiple subjects or conditions,
can further expand the data into additional analysis domains.
This can result in high-dimensional datasets, yet with a certain decoupling
between the domains.
Therefore, the intrinsic multiway structure of neural data~\cite{Erol2022} is
well-suited for representation as multiway arrays, or \emph{tensors}, which
provide a structured data representation that counteracts some of the drawbacks
resulting from this high dimensionality.

Machine learning techniques dealing with this datatype are referred to as tensor
methods.
Tensor methods can consider each analysis domain (tensor \emph{mode}) separately to
reduce a given problem into partial, per mode problems.
This has given rise to efficient dimensionality reduction techniques, such as
the Higher-Order Singular Value Decomposition
(\textsc{hosvd})~\cite{DeLathauwer2000,SoleCasals2018} or Canonical Polyadic Decomposition
(\textsc{cpd})\cite{Hitchcock1927,Nazarpour2006}.

The former are all examples of unsupervised techniques with applications in EEG
processing.Given the specific task-related output required in the BCI applications, however,
supervised feature extraction and machine learning techniques are often of
interest in this field.

One approach is to incorporate some assumptions of the tensor structure of the
data directly into the estimation of parameters of classic linear machine
learning methods, such as in Linear Discriminant Analysis or beamforming.
Advances have been made by leveraging the decoupling of the spatial and temporal
domains in EEG event-related potential (ERP) classification using Spatiotemporal
Discriminant analysis~\cite{Li2010,Zhang2013} or methods regularizing covariance
matrix estimation~\cite{Kerchove2022,Sosulski2022}.

A more structured approach to the same problem is to design a supervised
tensor dimensionality tensor method that optimizes discriminability between the
extracted features, as is the case for Higher Order Discriminant
Analysis (\textsc{hoda})~\cite{Yan2005,Phan2010,Froelich2018}.
These extracted features can subsequently be further classified, most commonly
using LDA or a support vector machine (SVM) to obtain predictions.
Variants of \textsc{hoda} have been applied to BCI problems such as
ERP~\cite{Onishi2012,Higashi2016} and motor imagery (MI)~\cite{Liu2015,Cai2021}
decoding.
Recent adaptations improve on these results by using suited objective
functions and regularization, such as in Higher-Order Spectral Regression
Discriminant Analysis~\cite{Jamshidi2017}, Spatiotemporal Linear
Feature Learning~\cite{Aghili2023}, and Oscillatory source Tensor Discriminant
Analysis~\cite{Jorajuria2022}.

The methods above adhere to the \textsc{tucker} tensor decomposition
structure, meaning that they reduce input tensors of size
$(D_1,D_2,\ldots,D_K)$ to a smaller tensor of size $(r_1,r_2,\ldots,r_K)$ with
each $r_k\leq D_k$, similar to \textsc{hosvd}.
While effective, other approached such as the \textsc{parafac} structure employed in
\textsc{cpd}, where a tensor is decomposed into a sum of rank-1 tensors,
might also be suitable to represent the neural data of interest.
Discriminant tensor features can also be extracted
in the \textsc{parafac} structure, for instance through manifold
optimization~\cite{Froelich2018}.

Nevertheless, the \textsc{parafac} structure might still not be able to
efficiently represent all relevant information in a compressed format.
The block-term tensor structure is a generalization of the \textsc{tucker} and
\textsc{parafac} structures, and can be calculated in an unsupervised way using
the Block-term Tensor Decomposition
(\textsc{btd})~\cite{DeLathauwer2008,DeLathauwer2008a,DeLathauwer2008b,Rontogiannis2021}.
\textsc{btd}, of which the \textsc{hosvd} and \textsc{cpd} are special cases,
represents a tensor a sum of \textsc{tucker} terms.
Research has shown that this more flexible structure can improve BCI performance
when adapted to supervised methods, such as in Higher-Order Parial Least
Squares~\cite{Camarrone2018} or Block-Term Tensor Regression (\textsc{bttr})~\cite{Faes2022,Faes2022b}
\textsc{bttr} has been adapted into a classification variant, named Block-Term
Tensor Classification (\textsc{bttc})~\cite{Camarrone2021}, but since features
are not directly optimized for class separability but rather regressed towards
a dummy independent variable, results can be improved upon and the method
cannot be extended to a multi-class setting.
Furthermore, structures employed in \textsc{hopls}, \textsc{bttdr} and
\textsc{bttc} are still more constrained than what could be achieved with a
full block-term tensor structured decomposition optimized for discriminability.

\todo{talk about spectrum-weighted tda~\cite{Huang2020}, criticize structure}

In this work, we propose the following contributions:
\begin{enumerate*}[label={\arabic*)}]
  \item first, we develop a forward model for \textsc{hoda} to reconstruct a
    given input tensor from the extracted features.
  \item This allows us to introduce a state-of-the-art BCI classification method based on the
    block-term tensor structure, named Block-Term Tensor Discriminant Analysis
    (\textsc{BTTDA}).
  \item Finallyn we evaluate this decoder and it's special
    \textsc{parafac}-structured case on an extensive benchmark of BCI
    datasets for ERP and MI decoding).
\end{enumerate*}

\section{Methods}

\subsection{Notation}
Tensors are indicated as bold underlined letters $\ten{X}$, matrices as bold
letters $\mat{U}$, fixed scalars as uppercase letters $K$ and variable
scalars as lower case letters $k$.
The $n^\text{th}$ sample of a tensor dataset with $N$ samples is written as
$\ten{X}(n)$.
A tensor $\ten{X}\in \mathbb{R}^{D_1\times D_2 \times \cdots \times D_K}$ can be unfolded in mode
$k$ to a matrix $\mat{X}_k\in\mathbb{R}^{(D_k\times\prod_{j\neq k}^K D_j)}$.
The tensor-matrix product of tensor $\ten{X}$ with matrix $\mat{U}$ along a
given mode $k$ is written as $\ten{X}\mpr{\mat{U}}{k}$. For ease of notation, let
$\ten{X}\mmpr{\mat{U}} =
	\ten{X}\mpr{\mat{U}}{1}\mpr{\mat{U}}{2}\cdots\mpr{\mat{U}}{K}$.
When skipping one of the modes $k$, this is
written as $\ten{X}\mmprs{\mat{U}}{k} =
	\ten{X}\mpr{\mat{U}}{1}\mpr{\mat{U}}{2}\cdots\mpr{\mat{U}}{k-1}\mpr{\mat{U}}{k+1}\ldots\mpr{\mat{U}}{K}$.
%The Kronecker product is noted as $\otimes$, covariance matrices are indicated  with $\mat{\Sigma}$.

\subsection{Higher-Order Discriminant Analysis (\textsc{hoda})}
Higher Order Discriminant Analysis (\textsc{hoda})~\cite{Phan2010} is a
supervised tensor
feature extraction technique. For a set of $N$ $K^{th}$ order input tensors
$\left\{\ten{X}(n)\in\mathbb{R}^{D_1\times D_2 \times \cdots \times
D_K}\right\}_n^N$, \textsc{hoda} finds projection matrices $\mat{U_k}$ for each mode $k$
that project a given $\ten{X}$ to a latent tensor
$\ten{G}\in\mathbb{R}^{r_1\times r_2\times\cdots\times r_K}$, usually with lower
dimensionality $(r_1\leq D_1,r_2\leq D_2,\ldots,r_K\leq D_K)$ using
tensor-matrix mode products
\begin{equation}
	\ten{G}  = \ten{X}\mmpr{\mat{U}}
	\label{eq:hoda-backward}
\end{equation}
visualized in Figure~\ref{fig:hoda-backward}.
\begin{figure}[t]
	\centering
	\input{figures/hoda_backward.tikz.tex}
	\caption{A visualization of the multilinear projection learnt by Higher Order
		Discriminant Analysis (\textsc{hoda}) for a dataset of $N$ second order tensors
		$\ten{X}$ of shape $(D_1,D_2)$.
		\textsc{hoda} finds projection matrices $\mat{U}_k$ such that maximal
		discriminability between classes is achieved in the projected latent tensors
		$\ten{G}$ with reduced dimensionality $(r_1,r_2)$.}
	\label{fig:hoda-backward}
\end{figure}
Analogous to the \textsc{hosvd}, \textsc{hoda} is a dimensionality
reduction decomposition that results in a dense latent tensor $\ten{G}$, and
imposes an orthogonality constraint on $\mat{U}_k$ to ensure uniqueness.
However, while for the \textsc{hosvd} decomposition the projection matrices
are chosen to minimize the reconstruction error, the projection matrices
$\mat{U}_k$ of \textsc{hoda} are optimized for maximal discriminability between
$\ten{G}(n)$ belonging corresponding classes with labels $c_n$.
This is a desirable property in a classification setting where samples are
high-dimensional tensors.

\textsc{hoda} optimizes discriminability in the Fisher sense, by optimizing the
Fisher ratio $\phi$ between the latent tensors $\ten{G}(n)$
\begin{equation}
  \phi = \frac{\sum_c^CN_c\left\lVert\bar{\ten{G}}(c)-\bar{\bar{\ten{G}}}\right\rVert_F^2}
	{\sum_n^N\left\lVert\ten{G}(n)-\bar{\ten{G}}(c_n)\right\rVert_F^2}
	\label{eq:fisher}
\end{equation}
for $C$ classes with each $N_c$ samples. $\bar{\ten{G}}(c)$ is the mean of
latent
tensors of class $c$, and $\bar{\bar{\ten{G}}}$ the mean of
these class mean latent tensors.
The goal is now to find the optimal projection matrices
\begin{equation}
  \left\{\mat{U}^*\right\} = \argmax_{\{\mat{U}\}}\phi
\end{equation}
which can be solved by the iterative algorithm in Algorithm~\ref{alg:hoda}.
\begin{algorithm}
  \caption{Higher-order Discriminant Analysis (\textsc{hoda}) backward solution}
  \label{alg:hoda}
  \input{include/alg_hoda.tex}
\end{algorithm}
To start, $\mat{U}_k$ are initialized to orthogonal matrices, e.g. as random
orthogonal matrices, by a per mode Singular Value Decomposition (\textsc{svd}),
or as the partial \textsc{hosvd} of all stacked tensors in the dataset.
At each iteration, the algorithm loops trough the modes and fixes all the
projection of all but $\mat{U}_k$ corresponding to mode $k$.
It then finds a partial latent tensor
\begin{equation}
  \ten{G}_{-k}=\ten{X}\mmprs{\mat{U}}{k}
\end{equation}
Subsequently, a new projection matrix $\mat{V}_k$ can be found analogous to Linear
Discriminant Analysis by constructing the within-class scatter matrix
\begin{equation}
  \mat{S}_{-k,\text{w}} = \sum_n^N\tilde{\mat{G}}_{-k,k}(n)\cdot\tilde{\mat{G}}_{-k,k}^\intercal(n)
\end{equation}
with $\tilde{\ten{G}}_{-k}(n) = \ten{G}_{-k}(n) - \bar{\ten{G}}_{-k}(c_n)$,
and the between-class scatter matrix
\begin{equation}
  \mat{S}_{-k,\text{b}} =
  \sum_c^CN_c\tilde{\bar{\mat{G}}}_{-k,k}(c)\cdot\tilde{\bar{\mat{G}}}_{-k,k}^\intercal(c)
\end{equation}
with $\tilde{\bar{\ten{G}}}_{-k}(c) = \bar{\ten{G}}_{-k}(c) - \bar{\bar{\ten{G}}}_{-k}$,
and solving for the $r_k$ leading eigenvectors in the eigenvalue problem
\begin{equation}
   \mat{S}_{-k,\text{b}}-\varphi_k\mat{S}_{-k,\text{w}} =
   \mat{V}_k\mat{\Lambda}\mat{V}_k^\intercal
\end{equation}
with $\varphi_k=\Tr\left(\mat{U}_k^\intercal\mat{S}_{-k,\text{b}}\mat{U}_k\right)/\Tr\left(\mat{U}_k^\intercal\mat{S}_{-k,\text{w}}\mat{U}_k\right)$
using the $\mat{U}_k$ obtained in the previous iteration.
Finally, the orthogonal transformation invariant projections $\mat{U}_k$
are obtained by calculating the
per-mode total scatter matrices
\begin{equation}
  S_{k,\text{t}} = \sum_n^N\mat{X}_k(n)\cdot\mat{X}_k^\intercal(n)
\end{equation}
and finding the $r_k$ leading eigenvectors of
\begin{equation}
  \mat{V}_k\mat{V}_k^\intercal\mat{S}_{k,\text{t}}\mat{V}_k\mat{V}_k^\intercal
  = \mat{U}_k\mat{\Lambda}\mat{U}_k^\intercal
\end{equation}
at each iteration~\cite{Wang2007}.
%and solving the generalized eigenvalue problem
%\begin{equation}
%  = \mat{\lambda}\mat{S}_{-k,\text{w}}\mat{U}_k
%\end{equation}
The iterative process halts after a fixed amount or iterations, or when the
update of each $\mat{U}_k$ is lower than a predetermined threshold.

To apply \textsc{hoda} in a classification setting, the projections
are first learned on a training dataset  with known class labels.
Next, these projections are used to extract latent tensors from the
tensors in the training dataset.
These latent training tensors are then reshaped (\emph{vectorized}) into feature vectors
$\vec{g} =  \vect(\ten{G})$ and used to train a decision classifier with the corresponding class labels.
At the evaluation stage the projections learned from the training dataset are
used to extract latent tensors from an unseen test dataset with unknown class
labels, which can also be vectorized and passed on to the trained decision
classifier.

To avoid overfitting and improve performance in low sample size settings, the
\textsc{hoda} problem can be regularized by shrinking the partial
within-class scatter matrices~\cite{Phan2010} with a shrinkage factor
$\alpha_k$ at each step such that the eigenvalue problem becomes
\begin{equation}
  \mat{S}_b^{(-k)} -
  \varphi\left[\left(1-\alpha_k\right)\mat{S}_{-k,\text{w}}+\alpha_k\mat{I}\right] =
  \mat{V}_k\mat{\Lambda}\mat{V}_k^\intercal
\end{equation}
Like in Linear Discriminant Analysis, the shrinkage parameter $\alpha_k$ can
also be estimated in a data driven way in \textsc{hoda}~\cite{Jorajuria2022},
e.g., using the Ledoit-Wolf procedure~\cite{Ledoit2003} at every iteration.


\subsection{A forward model for \textsc{hoda}}
As a prerequisite for our proposed \textsc{bttda} model, we must first find a
way to reconstruct the original data tensor $\ten{X}$ as accurately as possible
from $\ten{G}$ after dimensionality reduction.
In neuroimaging, this is referred to as a \emph{forward model}.
While a \emph{backward model} extracts latent sources or properties from the observed
data, which can be optimized for tasks like regression or classification or
informed by prior knowledge about source propagation, a forward model is a
generative model that expresses the observed data in function of some latent
properties or sources that are given. Forward models are useful for
interpretability and data compression.

The \textsc{hoda} projection in Equation~\ref{eq:hoda-backward} is an example
of a backward model.
A straightforward and computationally efficient candidate for a corresponding
forward model is
\begin{equation}
	\ten{X} = \ten{G}\mmpr{\mat{A}^\intercal} + \ten{\mathbfcal{E}}
	\label{eq:hoda-forward}
\end{equation}
with \emph{activation patterns} $\mat{A}_k \in \mathbb{R}^{D_k\times r_k}$
and error term $\ten{\mathbfcal{E}}$.
\begin{figure}[t]
	\centering
	\input{figures/hoda_forward.tikz.tex}
	\caption{The forward projection for \textsc{hoda}. Leveraging activation
		patterns $A_k$, the original tensor $\ten{X}$ can approximately be
		reconstructed from projected latent tensor $\ten{G}$. $A_k$ are chosen such
		that the variability captured in the latent tensor is maximally expressed in
		the reconstructed tensor and not in the error term.}
	\label{fig:hoda-forward}
\end{figure}

A forward model should make sure that reconstruction error is minimized and
variation captured in the latent tensor is maximally captured by the forward
projection term $\ten{G}\mmpr{\mat{A}^\intercal}$, and not by the error term
$\ten{\mathbfcal{E}}$~\cite{Haufe2014}.
Hence, we aim to minimize the expected value of the cross-covariance between
the noise term and the extracted latent tensors
\begin{equation}
  \left\{\mat{A}^*\right\} = \arg\min_{\{\mat{A}\}}\text{E}\left[\text{vec}\left({\ten{\mathbfcal{E}}(n)}\right)\text{vec}\left({\ten{G}(n)}\right)\right]_n
\end{equation}
or, equivalently~\cite{Parra2005,Haufe2014},
\begin{equation}
  \left\{\mat{A}^*\right\} = \arg\min_{\{\mat{A}\}}\sum_n^N\left(\ten{X}(n) - \ten{G}(n)\mmpr{\mat{A}}\right)^2
\end{equation}
This least squares tensor approximation problem can be solved efficiently using the
alternating least squares (ALS) algorithm~\cite{Comon2009},
iteratively fixing all but one of the activation patterns such that
\begin{equation}
  \mat{A}_k = \argmin_{\mat{A}_k}
  \sum_n^N\left[\mat{X}_k(n) -
  \mat{A}_k\left(\ten{G}(n)\mmprs{\mat{A}}{k}\right)_k\right]^2
\end{equation}
\todo{check multiplication dimensionalities here and in algorithm}
at every iteration, which can be solved directly by ordinary least squares.
The activation patterns are initialized to the weights $\{\mat{U}\}$ of the
backward model.
Similar to fitting the backward model, the iterative process for the forward
model halts after a fixed amount of iterations or when the update of each
$\mat{A}_k$ is lower than a predetermined threshold.
The full algorithm to determine the \textsc{hoda} forward projection is listed
in Algorithm~\ref{alg:hoda-fw}.
\begin{algorithm}
  \caption{Higher-order Discriminant Analysis (\textsc{hoda}) forward solution}
  \label{alg:hoda-fw}
  \input{include/alg_hoda_fw.tex}
\end{algorithm}


\subsection{Block-Term Tensor Discriminant Analysis (\textsc{bttda})}
After defining the forward model, we can construct our proposed block-term
tensor model. Assuming the latent tensors $\ten{G}$
obtained by the backward projection of \textsc{hoda} do not achieve perfect
class separation, the error term $\ten{\mathbfcal{E}}$ in
Equation~\ref{eq:hoda-forward} should still contain some discriminative
information, which can be exploited to improve classifier performance.
We thus extend the \textsc{hoda} feature extraction scheme with backward an
forward models defined in respectively Equations~\ref{eq:hoda-backward}
and~\ref{eq:hoda-forward} to Block-Term Tensor Discriminant Analysis
(\textsc{bttda}), with a forward model given by
\begin{align}
  \ten{X} & = \sum_b^B\ten{G}^{(b)}\mmpr{\mat{U}^{(b)}} + \ten{\mathbfcal{E}}
  \label{eq:bttda-forward}
\end{align}
which extracts $B$ latent tensors $\ten{G}^{(b)}$ from input tensor $\ten{X}$
until error term $\ten{\mathbfcal{E}}$ remains.
Figure~\ref{fig:bttda} further illustrates the \textsc{bttda} model.
\begin{figure*}[t]
	\centering
	\input{figures/bttda.tikz.tex}
  \caption{A forward model for Block-Term Tensor Discriminant Analysis
  (\textsc{bttda}). \textsc{bttda} can extract more features
 than \textsc{hoda} by iteratively finding a latent tensor $\ten{G}^{(b)}$ in a
 deflation scheme.
 The \textsc{hoda} backward projection is first applied. Next, the
 input data is reconstructed via the \textsc{hoda} forward model and the
 difference between the two is found.
 Finally, this process is repeated with this difference as input data, until a
 desired number of blocks $B$ has been found.}
  \label{fig:bttda}
\end{figure*}

Since \textsc{bttda} is specified as a forward model, a backward modelling
procedure is required which finds the latent tensors $\ten{G}^{(b)}$ given $\ten{X}$ for
\textsc{bttda} to be useful as a feature extraction method.
The extracted features represented by the latent tensors $\ten{G}^{(b)}$ can be
computed through a deflation scheme summarized in Algorithm~\ref{alg:bttda}.
\begin{algorithm}
  \caption{Block-term Tensor Discriminant Analysis (\textsc{bttda})}
  \label{alg:bttda}
  \input{include/alg_bttda.tex}
\end{algorithm}
For each block $b$, the latent tensor is extracted using the \textsc{hoda} backward
projection in from a residual error term
$\ten{\mathbfcal{E}}^{(b)}$
\begin{equation}
  \ten{G}^{(b)} = \ten{\mathbfcal{E}}^{(b)}\mmpr{\mat{U}^{(b)}}
\end{equation}
This residual error term is calculated by finding the difference between the
previous error and its reconstruction after backward and forward \textsc{hoda}
projection
\begin{equation}
  \ten{\mathbfcal{E}}^{(b+1)} = \ten{\mathbfcal{E}}^{(b)} - \ten{G}^{(b)}
  \mmpr{\mat{A}^{\intercal(b)}}
\end{equation}
with $\ten{\mathbfcal{E}}^{(1)}=\ten{X}$.

The resulting latent tensors can be vectorized and concatenated into
one single feature vector per input tensor
\begin{equation}
\vec{g}
  =\left[\vect(\ten{G}^{(1)})\
  \vect(\ten{G}^{(2)})\
  \cdots\
  \vect(\ten{G}^{(B)})\right]
\end{equation}
so that they can be classified in a similar manner to \textsc{hoda}.


\subsection{Model and feature selection}
Similar to the unsupervised \textsc{btd}, the performance of \textsc{bttda} is
heavily dependent on the rank $(r_1^{(b)}, r_2^{(b)}, \ldots,
r_K^{(b)})$ and on the number of blocks $B$.
If these are not kown a priori, i.e. if they cannot be set based on insights in the
data generation process, a model selection step is necessary in order to
determine what the optimal values for $r_k^{(b)}$ and $B$ are.

Furthermore, \textsc{hoda}, and by extension \textsc{bttda} can extract a substantial amount
of redundant features, which can be dropped after projection and before proceeding to the classification
step~\cite{Phan2010}.
Specifcally in \textsc{bttda} redundant features can stack up over the number of
blocks, hampering performance.
Relevant features can be retained by sorting all features in the feature vector
by their univariate Fisher score
\begin{equation}
  \phi(i) = \frac
  {\sum_c^C N_c \left(\bar{g}_i(c)-\bar{\bar{g}}_i\right)^2}
  {\sum_n^N \left(g_i(n)-\bar{g}_i(c_n)\right)^2}
\end{equation}
in decreasing order, and retaining the $L$ features with highest $\phi(i)$.

Hyperparameters $B$, $L$ and the individual
block ranks must be tuned to select a performant classification model.
These hyperparameters can be set through cross-validation, yet this can be
computationally expensive.
To reduce the computational cost of model selection,
Algorithm~\ref{alg:model-selection} proposes a heuristic model selection
algorithm that leverages cross-validation in a greedy way per block, to
iteratively find the optimal rank for the next block and the number of features
$L$, given the ranks of the previous block.
Area under the Receiver Operating Characteristic curve (ROC-AUC) for
classification of extracted feature vectors after feature selection is used as
cross-validation score.
\begin{algorithm}
  \caption{Greedy model selection}
  \label{alg:model-selection}
  \input{include/alg_model_selection.tex}
\end{algorithm}
Finally, the series of blocks can be truncated to the point with
highest validation score to determine $B$.

\section{Experiments}
\subsection{Datasets \& decoders}
We evaluated our proposed model in two off-line BCI decoding settings: ERP
paradigms and MI paradigms using the openly available MOABB benchmarking
datasets~\cite{Aristimunha2023}.
Details about the ERP and MI datasets are found in Table~\ref{tab:moabb_erp}
and Table~\ref{tab:moabb_mi} respectively.
Within-session classification performance was assessed using stratified 5-fold
cross-validation to calculate the area under the receiver operator
characteristic curve (ROC-AUC).

To use it as a decoder, \textsc{hoda} is paired with LDA to classify the
extracted feature (HODA+LDA), with hyperparameters $r_k$.
Similarly, we implemented BTTDA+LDA with the proposed \textsc{bttda} feature
extraction with hyperparameters $r_k^{(b)}$ for each block $b$ and the number of blocks
$1\leq B\leq16$.
Additionally, we also introduce PARAFACDA+LDA, which is the spacial case of
BTTDA+LDA where each $r_k^{(b)}=1$, with $B$ and $L$ as only hyperparameters.
Hyperparameters were determined separately for each fold using nested
stratified 5-fold cross-validation, and, for BTTDA+LDA, in conjunction with the
greedy model selection algorithm in Algorithm~\ref{alg:model-selection}
For HODA+LDA and the \textsc{hoda} blocks in BTTDA+LDA, we choose
$r=r_1=r_2=\ldots=r_K$ with possible values $1,2,4,8,\ldots,\min_kD_k$
to reduce computational cost.
\todo{mention z-scoring}
As comparison methods, we used the methods based on
Riemannian Geometry evaluated in~\cite{Chevallier2024} on
the MOABB datasets for the ERP datasets (ERPCov+MDM, ERPCov-SVD+MDM,
XDAWNCov+MDM, XDAWN+LDA,XDAWNCov+TS+SVM).
\todo{mention MI methods}
Implementation details can be found in~\cite{Chevallier2024}.
%Additionally, for the ERP paradigm, we compared our proposed methods to
%Toeplitz-LDA (tLDA)~\cite{Sosulski2022} since this algorithm generally yields state-of-the-art
%ERP classification performance.

\subsection{Event-Related Potentials (ERPs)}
ERPs are spatiotemporal features, with each sample forming a $2^\text{nd}$
order tensor with $K=2$ modes, representing EEG channels and time samples
per epoch.
EEG signals for the evaluated datasets were recorded at the sample rate given
by Table~\ref{tab:moabb-erp} and band-pass filtered between 1Hz
and 24Hz.
The signals were then cut into epochs starting from stimulus onset with a
dataset specific length given by Table~\ref{tab:moabb-erp}.
For tLDA, HODA+LDA, PARAFAC+LDA and BTTDA+LDA decoders, epochs were downsampled to 48Hz.

Table~\ref{tab:erp-score} lists the cross-validated ROC-AUC for all evaluated
decoders.
\begin{table*}[t]
  \footnotesize
  \centering
  %\input{include/score_erp.tex}
  \input{include/moabb_erp_score.tex}
  \caption{Scores for (list) were taken from \cite{Chevallier2024}}
  \label{tab:erp-score}
\end{table*}
Highest performance is reached with the proposed BTTDA+LDA or PARAFAC+LDA
decoders, except for datasets\todo{which datasets}
\todo{report average increase}
A Stouffer meta-analysis over all ERP datasets per pair of evaluated algorithms
(HODA+LDA, PARAFAC+LDA and BTTDA+LDA) of Wilcoxon signed rank tests per dataset,
reveals that\todo{what}

\subsection{Motor Imagery (MI)}
For MI, discriminatory information is represented in the EEG data as
Event-Related Synchronizations/Desynchronizations (ERS/Ds).
Contrary to the time domain analysis performed on ERPs, ERS/Ds are only
detectable in the time-frequency domain.
Hence, for the MI task, we will transform the EEG signal into the
time-frequency domain, forming $3^\text{d}$ order tensors, with $K=3$ modes
representing the channels, frequencies and time bins.

The MI datasets listed in Table~\ref{tab:moabb-mi} were first band-pass filtered between 8 and 32Hz and cut into
epochs with time windows as specified by Table~\ref{tab:moabb-mi}.
Next, time-frequency transformation was performed using a complex Morlet wavelet
convolution, with 23 wavelet frequencies logarithmically spaced between 8 and
32Hz.
The number of wavelet cycles $c$ varied with wavelet frequency $f$ as
$c=0.7*f$.
Features were extracted by taking the amplitude of the complex wavelet
coefficients, averaging each epoch along the time axis into time bins of
length $1/25$s and z-scoring.
\begin{table*}[t]
  \footnotesize
  \input{include/score_mi.tex}
  \caption{Scores for (list) were taken from \cite{Chevallier2024}}
  \label{tab:mi-score}
\end{table*}

\subsection{Block contribution}
To analyze the contribution of extra feature blocks extracted by {bttda} over
the first one found by \textsc{hoda}, we pick n ERP ($K=2$) dataset
(BNCI2014-008) and an MI ($K=3$) dataset (BNCI2014-001)
We report within-session ROC-AUC scores for training, validation and test data as a function
of the number of blocks increases, Figure~\ref{fig:blocks}.
Training and validation folds were taken from the model selection procedure,
for the test data, 20\% of epochs were held out of each session.
Additionally, the Normalized Mean Squared Error (NMSE) is reported for the
reconstructed from the truncated \textsc{bttda} decomposition
$\textstyle{\ten{X}_\text{rec}^{(B)}=\sum_b^B\ten{G}^{(b)}\mmpr{\mat{U}^{(b)}}}$.
NMSE is calculated as
\begin{equation}
  \nmse\left(\ten{X}, \ten{X}_\text{rec}^{(B)}\right) =
  \frac{\sum_n^N\left\lVert\ten{X}(n)-\ten{X}_\text{rec}^{(B)}(n)\right\rVert_F^2}
  {\sum_n^N\left\lVert\ten{X}(n)\right\rVert_F^2}
\end{equation}
\begin{figure*}[t]
  \input{figures/blocks.pgf}
  \caption{Normalized Mean Square Error (NMSE), (left), and difference in area
  under the receiver operating cure (ROC-AUC) for the training, validation and
  test data without feature selection (right), as a function of the number of BTTDA blocks $b$.
  While NMSE monotonically decreases for the evaluated datasets, better class
  separation will be achieved, but eventually overfitting occurs and validation
  and test scores will plateau or drop.
}
  \label{fig:blocks}
\end{figure*}


\section{Conclusion}
Summarize positive results
For which datasets does it work? For which does it not? (largs vs small, model
complexity)
compare with parafac

Figure~\ref{fig:blocks} shows that there is added value in finding extra blocks
over the first \textsc{hoda} block.
\todo{capitalization receiver operating characteristic}
While no proof is given, we notice that the Fisher score $\phi$ increases
monotonically with the number of blocks, indicating that solving the
\textsc{bttda} problem further optimizes the Fisher criterion feature
extraction problem.
On the other hand, NMSE \todo{NMSE equation, monotonically decreasing}
Together, these results point to the conclusion that \textsc{bttda} will
eventually model the full signal while still extracting features that are
maximally discriminant.
Eventually, the amount of blocks will reach a point of diminishing validation
score returns, when adding extra features to the decision classifier increases
its risk of overfitting instead of adding extra discriminatory information.
\begin{itemize}
  \item Decrease of MSE with blocks
  \item increase of f ratio with blocks
  \item increase of validation score with blocks
\end{itemize}

Discuss interpretability, include plots of first blocks

better memory/time complexity for same number of features than hoda

Despite favorable results in BCI decoding, the applications of the proposed
\textsc{bttda} model are limited mainly by the model selection approached used
to determine the individual block ranks.
While our proposed greedy model selection algorithm is a step in the right
direction, the high computational cost of setting hyperparameters through cross
validation can still hinder the portability of decoders relying on
\textsc{bttda}
Limitations: high computational cost of setting the parameters through cross
validation, greedy model
selection does not necessarily find most optimal and most interpretable
clusters of features. future efforts
should focus on automatic parameter setting e.g. using information criteria
such as in BTTR/C
does not work for specific datasets


Because of the flexibility and minimal assumptions (only tensor structure)
can equally be applied to other neuroimaging modalities (ECoG,fMRI, EMG)

%https://sci-hub.ru/https://ieeexplore.ieee.org/abstract/document/6287946
%https://www.sciencedirect.com/science/article/abs/pii/S0031320317301875
\printbibliography

\section*{Acknowledgements}
We thank the Flemish Supercomputer Center (VSC) and the High-Performance
Computing (HPC) center of the KU Leuven for allowing us to execute our
computational experiments on their systems.

\clearpage
\appendix

%\section{Proof of Theorem~\ref{the:ap}}
\onecolumn
\todo{check capitalization of event-related potential}
\section{ERP Benchmark datasets}
\begin{table*}[htp]
  \input{include/moabb_erp.tex}
  \caption{MOABB event-related potential benchmark datasets used for evaluation, with the number of
  subjects (\# Sub.), the number of EEG channels (\# Chan.), the number of trials
per data class (\# Trials/class), the epoch length (Epoch len.), the sampling
frequency (S. freq.) and the number of sessions per subject (\# Sessions).
Adapted from~\cite{Aristimunha2023} and~\cite{Chevallier2024}.}
  \label{tab:moabb-erp}
\end{table*}

\section{MI Benchmark datasets}
\begin{table*}[htp]
  \caption{MOABB motor imagery benchmark datasets used for evaluation, with the number of
  subjects (\# Sub.), the number of EEG channels (\# Chan.), the number of trials
per data class (\# Trials/class), the epoch length (Epoch len.), the sampling
frequency (S. freq.) and the number of sessions per subject (\# Sessions).
Adapted from~\cite{Aristimunha2023} and~\cite{Chevallier2024}.}
  \label{tab:moabb-mi}
\end{table*}

\end{document}
